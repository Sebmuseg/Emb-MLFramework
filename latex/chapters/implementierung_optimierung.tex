\chapter{Implementierung und Optimierung}
\label{chap:implementierung_optimierung}

Dieses Kapitel beschreibt die Implementierung und Optimierung von ML-Modellen für den Einsatz auf Embedded Systems detailliert. 
Die zu Verfügung Optimierungstechniken sowie der Prozess des Modelldeployments auf Geräten wie SPS, IPCs und Mikrocontrollern werden dabei erläutert. 
Der Schwerpunkt liegt auf der Anpassung der Modelle an die beschränkten Ressourcen dieser Systeme, um eine effiziente Echtzeitausführung sicherzustellen.

\section{Optimierung der ML-Modelle}
Da Embedded Systems nur begrenzte Rechenleistung, Speicher und Energie zur Verfügung haben, werden ML-Modelle entsprechend angepasst. 
Verschiedene Optimierungstechniken werden angewendet, um sicherzustellen, dass die Modelle effizient auf diesen Geräten laufen, 
ohne die Genauigkeit der Vorhersagen signifikant zu beeinträchtigen. Ein \textbf{Vergleich} verschiedener Techniken zeigt, 
dass die gewählte Methodik eine optimale Balance zwischen Modellgröße, Effizienz und Genauigkeit ermöglicht wie in \textbf{Tabelle \ref{tab:appendix_table} im Anhang \ref{appendix_a} beschrieben}.


    \section*{Erklärung und Begründung der gewählten Methoden}
    
    Quantisierung, Pruning und Modellkompression bieten eine ausgewogene Balance zwischen Modellgröße, Effizienz und Genauigkeit. Hier sind die Gründe für ihre Auswahl:
    
    \begin{itemize}
        \item \textbf{Quantisierung} eignet sich besonders für ressourcenbeschränkte Systeme, wie Mikrocontroller, und reduziert den Speicherbedarf erheblich. Mit Techniken wie Quantization-Aware Training (QAT) bleibt die Modellgenauigkeit weitgehend erhalten, wodurch eine effiziente Inferenzleistung sichergestellt wird.
        
        \item \textbf{Pruning} ist vorteilhaft bei tiefen neuronalen Netzen, da viele Verbindungen nur geringen Einfluss auf die Leistung haben. Diese Methode verringert die Modellkomplexität und -größe, ohne die Genauigkeit signifikant zu beeinträchtigen, insbesondere bei unstrukturiertem Pruning. Für spezifische Anforderungen eignet sich strukturiertes Pruning, das die Rechenkomplexität weiter reduziert.
        
        \item \textbf{Modellkompression} reduziert redundante Parameter und die Modellgröße erheblich, besonders bei komplexen Modellen. Sie gewährleistet schnelleren Zugriff auf Speicher und kürzere Ladezeiten, was sie ideal für Anwendungen auf Edge-Devices und ressourcenbeschränkten Systemen macht.
    
    \end{itemize}
    
Zusammenfassend lässt sich sagen, dass die Kombination dieser Techniken den Anforderungen an Embedded Systems gerecht wird, indem sie Speicherplatz, Rechenressourcen und Inferenzzeiten optimiert. 
Dabei bleiben die Vorhersagegenauigkeit und die Effizienz des Modells erhalten, was diese Methodenauswahl zur optimalen Lösung für industrielle Anwendungen macht.

\begin{description}
    \item[Quantisierung:] Die Quantisierung reduziert Modellgröße und Rechenanforderungen durch die Umwandlung der 32-Bit-Gleitkommazahlen der Modellgewichte 
    in 8-Bit-Ganzzahlen. Dies führt zu einer erheblichen Reduzierung der Modellgröße und Berechnungen.
    \begin{itemize}
        \item \textbf{Post-Training Quantization}: Nach dem Training wird eine Quantisierung durchgeführt, bei der die Gleitkommawerte der 
        Modellparameter auf Ganzzahlen reduziert werden. Diese Methode verringert die Speichernutzung und beschleunigt Berechnungen.
        \item \textbf{Quantization-Aware Training (QAT)}: Bei bestimmten Modellen wird das Quantization-Aware Training genutzt, 
        um die Effekte der Quantisierung bereits während des Trainings zu simulieren. Dadurch bleibt die Modellgenauigkeit nach der Quantisierung weitgehend erhalten.
    \end{itemize}

    \item[Pruning:] Pruning entfernt unnötige Gewichte und Verbindungen aus neuronalen Netzen, was die Komplexität und Größe der Modelle deutlich reduziert, 
    ohne die Vorhersagegenauigkeit wesentlich zu beeinträchtigen.
    \begin{itemize}
        \item \textbf{Unstrukturierter Pruning}: Durch das Entfernen einzelner, für die Modellleistung weniger wichtiger Verbindungen zwischen Neuronen wird die 
        Rechenkomplexität reduziert.
        \item \textbf{Strukturierter Pruning}: Ganze Neuronen oder Filter werden entfernt, um die Modelle schlanker und die Berechnungseffizienz zu steigern.
    \end{itemize}

    \item[Modellkompression:] Neben Pruning und Quantisierung dient Modellkompression dazu, die Gesamtgröße der Modelle zu verringern. 
    Hierbei werden redundante Parameter identifiziert und komprimiert, ohne die Vorhersageleistung wesentlich zu beeinflussen.

    \item[Optimierung für spezifische Hardware:] Da die Rechenkapazitäten von Embedded Systems variieren, erfolgt eine spezifische Optimierung der Modelle 
    für verschiedene Hardware-Plattformen. Unterschiede in der Anzahl der Prozessoren und der Speichergröße werden berücksichtigt, um optimale Leistung auf 
    Geräten wie SPS, IPCs und Mikrocontrollern zu gewährleisten.
    \begin{itemize}
        \item \textbf{TensorFlow Lite}: TensorFlow Lite optimiert Modelle für Mikrocontroller und andere ressourcenbeschränkte Geräte, 
        bietet Quantisierung und eine optimierte Inferenzlaufzeit.
        \item \textbf{ONNX Runtime}: ONNX Runtime ermöglicht die plattformübergreifende Optimierung und das Deployment der Modelle auf heterogenen Embedded-Plattformen.
    \end{itemize}
\end{description}

\section{Deployment der optimierten Modelle}

Nach der Optimierung der Modelle folgt das Deployment auf den Embedded-Systemen. Dieser Abschnitt beschreibt den Prozess der Modellbereitstellung 
und die spezifischen Herausforderungen, die bei der Echtzeitausführung auf Embedded-Hardware auftreten. Ein Vergleich der verschiedenen 
Deployment-Methoden belegt die Auswahl als optimale Lösung, da sowohl Echtzeitanforderungen als auch Ressourcenbeschränkungen berücksichtigt werden.

\begin{description}
    \item[Modellbereitstellung auf SPS und IPCs:] SPS- und IPC-Systeme dienen der Ausführung der optimierten Modelle in industriellen Umgebungen. 
    Da diese Systeme hinsichtlich Rechenleistung und Speicherkapazität variieren, wurden spezifische Deployment-Strategien implementiert, 
    um eine zuverlässige und performante Ausführung sicherzustellen. Die Auswahl der Deployment-Strategie richtet sich dabei nach dem Betriebssystem, 
    den verfügbaren Ressourcen und den Echtzeitanforderungen der Anwendung.
    \begin{itemize}
        \item \textbf{Lokale Ausführung}: Die Modelle werden direkt auf den Geräten ausgeführt, was die Latenz minimiert und Echtzeitanforderungen erfüllt.
        \item \textbf{Remote Deployment}: Für bestimmte Anwendungsfälle erfolgt das Deployment auf Edge-Geräten, die eng mit den SPS und IPCs verbunden sind. 
        Dadurch werden rechenintensive Aufgaben effizient ausgelagert und die Hauptsysteme entlastet.
    \end{itemize}

    \item[Tools für lokale Ausführung:] Die Bereitstellung auf SPS und IPCs erfordert häufig spezielle Anpassungen, da viele Geräte nicht dafür konzipiert sind, 
    komplexe Machine-Learning-Modelle zu verarbeiten. Zu den häufig eingesetzten Tools und Techniken zählen:
    \begin{itemize}
        \item \textbf{Apache TVM}: Ermöglicht die Kompilierung von Modellen für ressourcenbeschränkte Geräte und unterstützt unterschiedliche Hardwareplattformen. Apache TVM bietet eine hohe Inferenzgeschwindigkeit und ist besonders für Systeme geeignet, die keine Python- oder Docker-Unterstützung haben.
        \item \textbf{TinyML}: Speziell für Embedded-Systeme und Mikrocontroller entwickelt, bietet TinyML eine effiziente Laufzeitumgebung für Machine-Learning-Modelle auf Geräten mit extrem limitierten Ressourcen.
        \item \textbf{PyInstaller}: Da auf vielen IPCs mit Windows-Betriebssystem weder Python noch Docker installiert ist, ermöglicht PyInstaller die Umwandlung von Python-Anwendungen in ausführbare Dateien. Dadurch können die Modelle als Windows-Service bereitgestellt werden, ohne dass eine Python-Installation erforderlich ist.
        \item \textbf{Zephyr RTOS}: Dieses Echtzeitbetriebssystem ist besonders geeignet für Systeme, die robuste und schnelle Echtzeitberechnungen erfordern, wie SPS und Mikrocontroller. Zephyr RTOS bietet zudem eine breite Unterstützung für eingebettete Hardware und ermöglicht die zuverlässige Modellbereitstellung unter strengen Echtzeitanforderungen.
        \item \textbf{ONNX Runtime und TensorFlow Lite}: Diese Frameworks unterstützen eine effiziente Inferenz und sind ideal für ressourcenarme Systeme, die nicht für die Ausführung umfassender Machine-Learning-Frameworks ausgelegt sind. Sie bieten Quantisierung und Optimierung für spezifische Hardwarearchitekturen und eignen sich für die lokale Ausführung auf Embedded-Geräten.
    \end{itemize}

    \item[Anwendungsfälle und Zielumgebungen:] Die Auswahl der Tools für das Deployment richtet sich nach den Anforderungen des jeweiligen Anwendungsfalls. Einige wichtige Szenarien umfassen:
    \begin{itemize}
        \item \textbf{Industrie-PCs (Windows)}: Modelle werden hier häufig mit PyInstaller bereitgestellt, um als Windows-Service ausgeführt zu werden. Anwendungsbeispiele sind industrielle Bildverarbeitungssysteme zur Qualitätskontrolle, die hochauflösende Bilder analysieren und Anomalien erkennen.
        \item \textbf{Mikrocontroller (TinyML und Zephyr RTOS)}: Mikrocontroller werden oft in Überwachungssystemen eingesetzt, die kontinuierlich Sensordaten in Echtzeit analysieren. Anwendungen umfassen die Überwachung von Maschinenzuständen und die Anomaliedetektion in der Fertigung.
        \item \textbf{Edge-Devices (Apache TVM und ONNX Runtime)}: Diese Geräte eignen sich für datenintensive Anwendungen, wie z.B. das Erfassen und Verarbeiten von Echtzeitbildern in Automated Optical Inspection (AOI)-Systemen.
    \end{itemize}

    \begin{table}[h!]
        \centering
        \caption{Optimierte Modellbereitstellung auf Embedded-Systemen in der industriellen Fertigung}
        \resizebox{\textwidth}{!}{ % Resize the table to fit within text width
            \begin{tabular}{|l|l|p{3.5cm}|l|l|}
                \hline
                \textbf{Zielgerät} & \textbf{Betriebssystem} & \textbf{Tools} & \textbf{Anwendungsfall} & \textbf{Technische Anforderungen} \\ \hline
                IPC & Windows & 
                \parbox{3.5cm}{\centering \vspace{0.2cm} PyInstaller \\ ONNX Runtime \vspace{0.2cm}} & 
                Bildverarbeitung, Qualitätskontrolle & 
                16 GB RAM, CPU > 2 GHz, keine Python-Umgebung erforderlich \\ \hline
                SPS & Linux-basiert / proprietär & 
                \parbox{3.5cm}{\centering \vspace{0.2cm} ONNX Runtime \\ Zephyr RTOS \vspace{0.2cm}} & 
                Maschinenüberwachung, prädiktive Wartung & 
                2-4 GB RAM, Echtzeitanforderungen, geringer Stromverbrauch \\ \hline
                Mikrocontroller & TinyML / Zephyr RTOS & 
                \parbox{3.5cm}{\centering \vspace{0.2cm} TinyML \\ Zephyr RTOS \vspace{0.2cm}} & 
                Anomaliedetektion, Sensordatenanalyse & 
                < 1 GB RAM, < 1 GHz, geringer Stromverbrauch \\ \hline
                Edge-Device & Linux / proprietär & 
                \parbox{3.5cm}{\centering \vspace{0.2cm} Apache TVM \\ ONNX Runtime \vspace{0.2cm}} & 
                Optische Inspektion (AOI), Echtzeitbilderkennung & 
                8 GB RAM, GPU-Unterstützung, schnelle Bildverarbeitung \\ \hline
            \end{tabular}
        }
        \label{tab:optimized_deployment}
    \end{table}


    \item[Verwaltung des Modelldepots:] 
Ein zentrales Modelldepot ermöglicht eine effiziente Verwaltung und Bereitstellung verschiedener Modellversionen. Durch die zentrale Speicherung und Bereitstellung auf den Embedded-Systemen wird eine unkomplizierte 
Aktualisierung und Versionskontrolle der Modelle gewährleistet. Der Vergleich zwischen dezentralen und zentralen Speicherstrategien zeigt, dass das zentrale Depot die Verwaltung erheblich vereinfacht und gleichzeitig 
die Konsistenz der Modellversionen sicherstellt.

\textbf{MLflow als zentrales Tool für die Modellverwaltung} \\
MLflow ist ein Open-Source-Tool zur Verwaltung des Machine-Learning-Lebenszyklus und umfasst vier zentrale Komponenten:
\begin{itemize}
    \item \textbf{MLflow Tracking}: Erfasst und speichert alle Parameter, Metriken und Artefakte von Modellen während des Trainings. Dies ermöglicht eine umfassende Dokumentation der Experimente und vereinfacht die Reproduzierbarkeit.
    \item \textbf{MLflow Projects}: Standardisiert die Struktur von ML-Projekten und fördert so die Wiederverwendbarkeit und den einheitlichen Aufbau.
    \item \textbf{MLflow Models}: Unterstützt die Speicherung von Modellen in einem plattformübergreifenden Format, wodurch die Bereitstellung in verschiedenen Umgebungen vereinfacht wird.
    \item \textbf{MLflow Model Registry}: Ein zentrales Modellregister, das eine Versionskontrolle, Modellbereitstellung und Übergangsphasen (z.B. „Staging“, „Production“) für Modelle bietet.
\end{itemize}

\textbf{Beispiel für den Einsatz von MLflow im Framework-Workflow} \\
Die Nutzung von MLflow innerhalb des Frameworks ermöglicht eine nahtlose Verwaltung und Bereitstellung von Modellen. Der folgende Workflow beschreibt den Einsatz von MLflow zur Verwaltung und Aktualisierung von Modellen in einer industriellen Umgebung:

\begin{enumerate}
    \item \textbf{Modelltraining und Experiment-Tracking}:
    Während des Modelltrainings werden alle relevanten Parameter und Metriken (z.B. Genauigkeit, Verlustfunktion) durch MLflow Tracking erfasst. Dies ermöglicht eine umfassende Dokumentation der Experimente und erleichtert die Vergleichbarkeit von Modellen.

    \item \textbf{Modellregistrierung und Versionierung}:
    Nach dem Training wird das optimierte Modell in der MLflow Model Registry gespeichert und versioniert. Hierbei erhält das Modell eine eindeutige Versionsnummer und kann in verschiedene Phasen (z.B. „Staging“ oder „Production“) überführt werden.

    \item \textbf{Deployment des Modells auf Embedded-Systemen}:
    Das Modell wird aus der MLflow Registry abgerufen und auf das Embedded-System bereitgestellt. Für ein Embedded-System ohne Python-Umgebung kann das Modell z.B. durch PyInstaller in eine ausführbare Datei konvertiert oder mit TensorFlow Lite kompiliert werden.

    \item \textbf{Überwachung und kontinuierliche Aktualisierung}:
    Während der Laufzeit werden Performance-Daten (z.B. Latenzzeiten und Genauigkeit) gesammelt und regelmäßig in MLflow gespeichert. Basierend auf diesen Daten können Anpassungen vorgenommen und neue Modellversionen in der Registry hinterlegt werden, um eine kontinuierliche Verbesserung zu gewährleisten.
\end{enumerate}

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{model-dev-lifecycle.png} 
    \caption{Modell-Entwicklungslebenszyklus mit MLflow \cite{MLFlow}} 
    \label{fig:architecture}
\end{figure}

    \item[Echtzeitausführung und Priorisierung:] Viele industrielle Anwendungen stellen strenge Echtzeitanforderungen. Um die Ausführung innerhalb der definierten 
    Zeitrahmen sicherzustellen, erfolgt eine Aufgabenpriorisierung und kontinuierliche Überwachung der Laufzeitleistung.
    \begin{itemize}
        \item \textbf{Task-Priorisierung}: Durch Priorisierung der Modellvorhersagen werden zeitkritische Aufgaben bevorzugt behandelt, wodurch die Einhaltung der Echtzeitanforderungen sichergestellt wird.
        \item \textbf{Überwachung der Ausführungszeit}: Die Laufzeitleistung wird kontinuierlich überwacht, um zu gewährleisten, dass die vorgegebenen Echtzeitanforderungen erfüllt werden. 
        Diese Überwachung umfasst das regelmäßige Protokollieren von Laufzeiten und die dynamische Anpassung der Ressourcennutzung.
    \end{itemize}
    
    \item[Tests und Validierung der Laufzeitleistung:] Nach dem Deployment der optimierten Modelle werden umfangreiche Tests zur Validierung der Laufzeitleistung und der Vorhersagegenauigkeit durchgeführt. 
    Diese Tests belegen, dass die gewählte Methodik für die Echtzeitanforderungen geeignet ist.
    \begin{itemize}
        \item \textbf{Unit-Tests}: Zur Validierung der Funktionalität und der korrekten Modellvorhersagen.
        \item \textbf{Performance-Benchmarks}: Die Ausführungszeiten werden gemessen und mit den festgelegten Echtzeitanforderungen verglichen, um die Effizienz der Optimierungen zu bestätigen.
        \item \textbf{Hardware-in-the-Loop (HIL)-Tests}: Diese Tests simulieren reale Hardware-Interaktionen, um die Robustheit und Stabilität des Systems unter industriellen Bedingungen zu überprüfen. 
        Die Tests werden regelmäßig wiederholt, um die Reproduzierbarkeit und Zuverlässigkeit der Ergebnisse sicherzustellen.
    \end{itemize}
\end{description}
\section{Zusammenfassung}

Die Implementierung und Optimierung der Machine-Learning-Modelle für Embedded Systems erfolgt durch den gezielten Einsatz von Techniken wie Quantisierung, 
Pruning und Modellkompression. Diese Methodiken werden aufgrund ihrer Effizienz und Eignung für ressourcenbeschränkte Umgebungen ausgewählt und gegenüber 
alternativen Ansätzen geprüft. Quantisierung und Pruning bieten eine optimale Balance zwischen Modellgenauigkeit und reduzierten Speicheranforderungen, 
während die Modellkompression eine signifikante Verringerung der Modellgröße bei minimalem Einfluss auf die Leistung gewährleistet.

Die optimierten Modelle sind erfolgreich auf verschiedenen Hardwareplattformen – einschließlich SPS, IPCs und Mikrocontrollern – bereitgestellt und in 
Echtzeitanwendungen integriert. Die Auswahl dieser Methodiken erweist sich als optimal, da sie eine flexible Anpassung an die spezifischen 
Ressourcenanforderungen der jeweiligen Hardware ermöglichen und gleichzeitig die notwendige Modellgenauigkeit beibehalten.

Durch die kontinuierliche Überwachung und Priorisierung der Aufgaben stellt das Framework sicher, dass die Modelle die strengen Echtzeitanforderungen der 
industriellen Umgebung zuverlässig erfüllen. Die Kombination aus Optimierungstechniken und spezifischen Deployment-Strategien bildet eine robuste Grundlage 
für den langfristigen Einsatz der Machine-Learning-Modelle in ressourcenbeschränkten industriellen Anwendungen.