\chapter{Entwicklung des Frameworks}
\label{chap:entwicklung_framework}
In diesem Kapitel werden die Architektur und die Entwicklung des Frameworks beschrieben, das auf die Optimierung von ML-Modellen für ressourcenbeschränkte Umgebungen 
wie Embedded und Edge-Devices ausgerichtet ist. Die Konzeption des Frameworks basiert auf einer umfassenden Literaturrecherche und Anforderungsanalyse, die den wissenschaftlichen 
und praktischen Hintergrund zur Gestaltung des Systems liefert.

Die \textbf{Literaturrecherche} identifiziert bestehende Ansätze und Techniken zur Optimierung von ML-Modellen in Embedded Systems. Zunächst werden zentrale Forschungsfragen 
formuliert, die die Herausforderungen und potenziellen Lösungen für ressourceneffiziente ML-Modelle in industriellen Anwendungen adressieren. In renommierten wissenschaftlichen 
Datenbanken wie IEEE Xplore \cite{IEEE_Xplore}, ACM Digital Library \cite{10.1145/3674912.3674918} und SpringerLink \cite{farbiz2023knowledge} erfolgt eine strukturierte Suche mit 
Begriffen wie „Embedded Systems“, „Machine Learning Optimierung“, „Edge Computing“ und „Echtzeitanforderungen“. Die Inklusions- und Exklusionskriterien gewährleisten die Relevanz 
der Artikel; insgesamt werden 50 Artikel analysiert, von denen 30 als besonders geeignet für die Konzeption des Frameworks eingestuft werden. Die in der Literatur identifizierten 
Ansätze werden nach Relevanz bewertet und bilden die Grundlage für die zentralen Anforderungen und Architekturprinzipien des Frameworks.

Ergänzend zur Literaturrecherche erfolgt eine \textbf{Anforderungsanalyse}, um die spezifischen Bedürfnisse der industriellen Partner zu berücksichtigen. In Interviews und Workshops 
mit Partnern, die Embedded Systems in der Produktion einsetzen, werden Anforderungen wie Rechenleistung, Echtzeitfähigkeit und Modellflexibilität dokumentiert. Zudem werden 
technische Spezifikationen der eingesetzten Hardware (z. B. SPS und IPC) untersucht, um spezifische Hardwareeinschränkungen und Schnittstellenanforderungen zu berücksichtigen. 
Die Anforderungen werden in technische und funktionale Anforderungen kategorisiert und priorisiert, um eine Balance zwischen Modellkomplexität, Laufzeitleistung und Speicherverbrauch 
zu erreichen.

Auf Basis dieser Ergebnisse ist das Framework als \textbf{modulares System} konzipiert, das Flexibilität, Erweiterbarkeit und Wartungsfreundlichkeit gewährleistet. 
Es unterstützt sowohl neuronale Netze als auch klassische ML-Modelle und besteht aus mehreren Hauptmodulen mit jeweils spezifischen Funktionen. Die wichtigsten Module 
umfassen das \textit{Modul für Modelloptimierung}, das Techniken wie Pruning, Quantisierung und Modellkompression zur Vorbereitung der Modelle auf ressourcenbeschränkte 
Geräte einsetzt, sowie das \textit{Modul für Evaluation}, das die Bewertung und Evaluation der Modellen direkt auf dem Embedded Device durchführt. 
Zudem bietet das \textit{Modul für das Modelldepot} eine zentrale Verwaltung und Bereitstellung der ML-Modelle und unterstützt unterschiedliche Deployment-Strategien, 
darunter das lokale Deployment auf Embedded Devices und das Remote-Deployment über Edge-Server.

Weitere zentrale Komponenten sind das \textit{Modul für Echtzeitausführung}, das die Einhaltung festgelegter Zeitgrenzen für die Ausführung der Modelle sicherstellt 
und eine effiziente Aufgabenpriorisierung ermöglicht, sowie das \textit{Modul für Überwachung und Logging}, das die Systemleistung überwacht und Leistungskennzahlen wie CPU-Auslastung, 
Speichernutzung und Model performance protokolliert. Die modulare Struktur des Frameworks fördert die Anpassbarkeit an verschiedene industrielle Anforderungen und bietet 
eine Grundlage für die langfristige Nutzbarkeit und Erweiterbarkeit des Systems.

Die Architektur des Frameworks umfasst eine \textbf{flexible API}, die eine einfache Integration verschiedener ML-Modelle ermöglicht und die Kommunikation zwischen Embedded Systems 
und Edge-Servern effizient gestaltet. Diese API stellt wesentliche Funktionen bereit, um ML-Modelle zu laden, zu konfigurieren und deren Vorhersagen in Echtzeit bereitzustellen. 
Zudem erlaubt sie die flexible Aktualisierung und Bereitstellung neuer Modelle, was die langfristige Anpassbarkeit des Frameworks unterstützt.

Um den Anforderungen industrieller Umgebungen gerecht zu werden, ist die API so konzipiert, dass sie mit einer Vielzahl von Protokollen, darunter MQTT, HTTP und OPC-UA, kompatibel ist. 
Diese Vielseitigkeit in der Protokollunterstützung ermöglicht eine nahtlose Datenübertragung und erleichtert die Integration in bestehende industrielle Netzwerke. 
Durch diese Kommunikationsschnittstellen wird sichergestellt, dass das Framework eine robuste und skalierbare Lösung für verschiedene Einsatzszenarien in der industriellen Produktion bietet.

Für eine detaillierte Übersicht der API-Funktionen und zur Veranschaulichung der Funktionalität ist im \textbf{Anhang} eine Darstellung der \textit{Swagger UI} enthalten. 
Diese dokumentiert die API, zeigt alle verfügbaren Endpunkte und ermöglicht es, diese interaktiv zu testen. Die Swagger UI, erstellt mithilfe von \texttt{FastAPI} und \texttt{Swagger}, 
bietet eine benutzerfreundliche und umfassende Visualisierung der API-Struktur. Sie kann in \autoref{fig:Swagger_UI} im Anhang gefunden werden.

\section{Optimierung der ML-Modelle für ressourcenbeschränkte Umgebungen}

Ein wesentlicher Aspekt der Framework-Entwicklung umfasst die Anpassung der ML-Modelle an die beschränkten Ressourcen von Embedded Systems. 
Diese Optimierung erfolgt durch verschiedene Techniken, die im Folgenden beschrieben werden:

\begin{description}
    \item[Pruning] wird eingesetzt, um unnötige Verbindungen in tiefen neuronalen Netzen zu entfernen und dadurch die Komplexität der Modelle zu reduzieren \cite{10.1145/3664647.3681449}. 
    Diese Technik senkt den Speicherbedarf erheblich, während die Modellgenauigkeit weitgehend erhalten bleibt.

    \item[Quantisierung] reduziert die Modellgewichte von 32-Bit-Floating-Point-Zahlen auf 8-Bit-Ganzzahlen \cite{10.1145/3368826.3377912}. Durch diese Technik werden sowohl der 
    Speicherbedarf als auch die Berechnungsanforderungen des Modells gesenkt, was insbesondere für Mikrocontroller und Embedded Systems in der industriellen Fertigung von Vorteil ist.

    \item[Modellkompression] wird angewendet, um die Modellgröße weiter zu reduzieren, indem redundante oder weniger wichtige Parameter komprimiert werden \cite{10.1145/3613904.3642109}. 
    Diese Technik minimiert nicht nur den Speicherverbrauch, sondern auch die Ladezeiten der Modelle.
\end{description}

Durch den kombinierten Einsatz dieser Optimierungstechniken wird sichergestellt, dass die ML-Modelle effizient auf ressourcenbeschränkten Geräten ausgeführt werden können, was ihre 
Anwendbarkeit in industriellen Umgebungen verbessert.

\section{Anpassung an verschiedene Embedded- und Edge-Geräte}

Das Framework ist für den Einsatz auf einer Vielzahl von Hardwareplattformen konzipiert, die von Mikrocontrollern bis hin zu leistungsstarken Edge-Geräten reichen. 
Dabei wird sichergestellt, dass das Framework flexibel genug ist, um auf verschiedenen Geräten mit unterschiedlichen Ressourcenanforderungen zuverlässig zu funktionieren.

\begin{description}
    \item[Unterstützte Hardwareplattformen:] Das Framework wurde gezielt für mehrere Hardwareplattformen optimiert, die spezifische Anforderungen und Eigenschaften aufweisen:
    \begin{itemize}
        \item \textbf{Speicherprogrammierbare Steuerungen (SPS)} sind für den Einsatz in der industriellen Automatisierung vorgesehen und erfordern robuste, echtzeitfähige ML-Modelle.
        \item \textbf{Industrie-PCs (IPCs)} bieten im Vergleich zu SPS mehr Rechenleistung und Speicher, wodurch sie sich für komplexere ML-Modelle und anspruchsvolle 
        Datenverarbeitungsaufgaben eignen.
        \item \textbf{Mikrocontroller} sind stark ressourcenbeschränkte Geräte, auf denen besonders kleine und optimierte ML-Modelle effizient ausgeführt werden müssen.
        \item \textbf{Edge-Devices} stellen leistungsstarke Plattformen dar, die zur Vorverarbeitung großer Datenmengen und zur Ausführung komplexer ML-Modelle verwendet werden können.
    \end{itemize}

    \item[Anpassung an Ressourcenprofile:] Da jede Hardwareplattform unterschiedliche Anforderungen an Speicher, Rechenleistung und Energieverbrauch stellt, bietet das 
    Framework Mechanismen, um die Modelle und ihre Ausführungsumgebung dynamisch an die Ressourcen der jeweiligen Zielplattform anzupassen. Zu diesen Mechanismen gehört die 
    automatische Auswahl geeigneter Optimierungstechniken, wie etwa Quantisierung oder Pruning, basierend auf den verfügbaren Ressourcen der jeweiligen Plattform.
\end{description}

\section{Zusammenfassung der Framework-Entwicklung}

Das entwickelte Framework ist darauf ausgelegt, ML-Modelle effizient auf Embedded- und Edge-Geräten auszuführen. Die modulare Architektur sowie die umfassende Unterstützung 
von Optimierungstechniken wie Pruning, Quantisierung und Modellkompression gewährleisten den Einsatz des Frameworks in verschiedenen ressourcenbeschränkten Umgebungen. 
Die flexible API und die anpassbare Struktur ermöglichen eine einfache Integration und Verwaltung unterschiedlicher ML-Modelle. Mit diesen Merkmalen bildet das Framework eine 
zukunftssichere Lösung, die speziell für Anforderungen in der industriellen Fertigung entwickelt wurde.