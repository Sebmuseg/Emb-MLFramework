\chapter{Methodik}
\label{chap:methodik}

n diesem Kapitel wird die Methodik beschrieben, die für die Entwicklung des Frameworks verwendet wird, das ML-Modelle für den Einsatz auf Embedded Systems optimiert. 
Dazu gehören die Durchführung der \textbf{Literaturrecherche}, die \textbf{Anforderungsanalyse} sowie die Auswahl der \textbf{Entwicklungsumgebung}, die verwendeten 
\textbf{Optimierungstechniken} und die \textbf{Implementierungsschritte} für das Framework. Der Fokus liegt auf der effizienten Ausführung von ML-Modellen auf 
ressourcenbeschränkten Geräten wie SPS und IPC in der industriellen Fertigung.

\section{Literaturrecherche und Anforderungsanalyse}

Um eine fundierte Basis für die Entwicklung des Frameworks zu schaffen, werden sowohl eine strukturierte Literaturrecherche als auch eine Anforderungsanalyse durchgeführt.
\subsection{Literaturrecherche}

Die Literaturrecherche dient dazu, aktuelle Ansätze und Techniken im Bereich der Embedded Systems und des Machine Learning in der Industrie 4.0 zu identifizieren. 
Folgende Schritte werden befolgt:
\begin{quote}
\textbf{Prozess:}
\begin{enumerate}
\item Definition der Forschungsfrage: Die zentrale Fragestellung zielt darauf ab, die Herausforderungen und Lösungsansätze zur effizienten Implementierung von ML-Modellen auf Embedded Systems in industriellen Anwendungen zu identifizieren.
\item Festlegung der Suchstrategie: Relevante wissenschaftliche Datenbanken wie IEEE Xplore, ACM Digital Library und SpringerLink werden genutzt. Suchbegriffe wie „Embedded Systems“, „Machine Learning“, „Edge Computing“ und „Industrie 4.0“ bilden die Grundlage für die Suche.
\item Filterung und Auswahl: Durch spezifische Inklusions- und Exklusionskriterien, wie Veröffentlichungsjahr und Relevanz für industrielle Anwendungen, werden die Suchergebnisse eingegrenzt. Insgesamt werden über 50 Fachartikel und Berichte analysiert, von denen 30 für die weitere Untersuchung relevant sind.
\item Analyse und Synthese: Die ausgewählten Artikel werden systematisch analysiert und die Ergebnisse in Bezug auf Schlüsselthemen und Herausforderungen zusammengefasst.
\end{enumerate}
\end{quote}
\subsection{Anforderungsanalyse}

Parallel zur Literaturrecherche wird eine Anforderungsanalyse durchgeführt, um spezifische Anforderungen der Industriepartner zu berücksichtigen. Diese Analyse folgt einem strukturierten Ansatz:
\begin{quote}
\textbf{Prozess:}
\begin{enumerate}
\item \textbf{Interviews und Workshops mit Industriepartnern}: Zur Identifikation praxisrelevanter Anforderungen werden Interviews und Workshops mit Industriepartnern durchgeführt, die Embedded Systems im Produktionsumfeld einsetzen. Hier werden Anforderungen bezüglich Rechenleistung, Energieeffizienz, Echtzeitanforderungen und Modellflexibilität identifiziert.
\item \textbf{Analyse der technischen Dokumentationen}: Technische Spezifikationen der verwendeten Hardware (z. B. SPS und IPC) werden untersucht, um Hardware-Einschränkungen und Schnittstellenanforderungen zu berücksichtigen.
\item \textbf{Kategorisierung der Anforderungen}: Die identifizierten Anforderungen werden in technische und funktionale Anforderungen unterteilt. Technische Anforderungen beziehen sich auf die Hardwareeinschränkungen und notwendige Optimierungstechniken, während funktionale Anforderungen Aspekte wie Echtzeitfähigkeit und Robustheit umfassen.
\item \textbf{Priorisierung}: Eine Priorisierung der Anforderungen wird vorgenommen, um sicherzustellen, dass die wichtigsten Kriterien wie Echtzeitanforderungen und Effizienz bei der Entwicklung des Frameworks berücksichtigt werden.
\end{enumerate}
\end{quote}
Priorisierung: Eine Priorisierung der Anforderungen wurde vorgenommen, um sicherzustellen, dass die wichtigsten Kriterien wie Echtzeitanforderungen und Effizienz bei der Entwicklung des Frameworks berücksichtigt werden.

\section{Framework-Entwurf}

Der erste Schritt zur Entwicklung des Frameworks ist die Definition der Anforderungen basierend auf den spezifischen Einschränkungen von Embedded Systems. 
Diese Anforderungen werden in Zusammenarbeit mit Industriepartnern und durch eine umfassende Literaturrecherche zu bestehenden Lösungen festgelegt. Hierbei werden nicht 
nur die Optimierung und Konfiguration der ML-Modelle berücksichtigt, sondern auch Aspekte wie die Datenverarbeitung, Priorisierung von Aufgaben, Logging und das Deployment 
von Modellen auf unterschiedlichen Zielplattformen. Die Schwerpunkte des Frameworks liegen auf:
\begin{itemize}
    \item \textbf{Reduzierung der Modellgröße}: Um die effiziente Ausführung von ML-Modellen auf ressourcenbeschränkten Embedded Systems wie SPS oder IPCs zu ermöglichen, 
    wird die Modellgröße reduziert, ohne dass die Genauigkeit signifikant leidet. Dies ist besonders für Echtzeitanwendungen entscheidend.   
    \item \textbf{Datenverarbeitung und Vorverarbeitung}: Neben der Modelloptimierung ist eine effiziente Datenverarbeitung erforderlich. Die Vorverarbeitung von Sensordaten 
    erfolgt in Echtzeit und filtert irrelevante Informationen, um sicherzustellen, dass nur relevante Daten für Modellvorhersagen genutzt werden. Dies unterstützt die Fähigkeit, 
    Entscheidungen in Echtzeit zu treffen.
   
    \item \textbf{Deployment und Aktualisierung der Modelle}: Eine zentrale Herausforderung in industriellen Umgebungen ist das flexible Deployment von ML-Modellen auf 
    verschiedenen Geräten (SPS, IPCs etc.). Das Framework ermöglicht eine einfache Bereitstellung neuer oder aktualisierter Modelle, ohne den Produktionsprozess zu stören, 
    und bietet ein robustes Update-Management für nahtlose Integration in bestehende Systeme.
  
    \item \textbf{Priorisierung von Aufgaben}: In Produktionsumgebungen ist die Priorisierung bestimmter Aufgaben zur Erfüllung von Betriebsanforderungen notwendig. 
    Das Framework bevorzugt daher priorisierte ML-Tasks gegenüber weniger kritischen Aufgaben, was insbesondere bei begrenzten Ressourcen und parallelem Prozessbetrieb entscheidend ist.
   
    \item \textbf{Logging und Überwachung}: Um die Systemausführung und -wartung zu unterstützen, integriert das Framework ein umfassendes Logging- und Überwachungssystem. 
    Dieses protokolliert Modellvorhersagen sowie Systemstatusinformationen (z.B. Ressourcenauslastung, Latenzen) und dient der Identifikation von Fehlern, der 
    Leistungsüberwachung und der langfristigen Wartung des Systems.
  
    \item \textbf{Sicherstellung der Echtzeitfähigkeit}: Da viele industrielle Anwendungen Echtzeitanforderungen erfordern, stellt das Framework sicher, dass 
    Vorhersagen innerhalb vorgegebener Zeitgrenzen getroffen werden. Dies erfordert eine optimierte Laufzeitleistung und die Einhaltung zeitlicher Anforderungen des Produktionsprozesses.
   
    \item \textbf{Einfache Integration in bestehende industrielle Steuerungssysteme}: Das Framework ist so konzipiert, dass es sich problemlos in bestehende Steuerungsarchitekturen 
    (wie IPC oder SPS) integriert, um den Einsatz ohne grundlegende Änderungen an der Infrastruktur zu ermöglichen.
\end{itemize}

Das Framework wird als Middleware entwickelt, die als Adapter zwischen den ML-Modellen und den Zielsystemen (SPS, IPC) fungiert. Es unterstützt verschiedene Optimierungstechniken, 
um die Ausführung der Modelle auf hardwarebeschränkten Systemen zu ermöglichen.

\section{Programmiersprachen und Entwicklungsumgebungen}

\subsection{Python}
Python ist eine der am häufigsten verwendeten Programmiersprachen für Machine Learning und bietet eine Vielzahl von Bibliotheken und Frameworks, 
die für industrielle Anwendungen und Embedded-Modelle nützlich sind. Es ist bekannt für seine einfache Handhabung und große Community, 
was es zur ersten Wahl für viele ML-Entwickler macht.

\begin{itemize}
    \item \textbf{TensorFlow und TensorFlow Lite}: TensorFlow bietet eine leistungsstarke Umgebung für die Entwicklung von neuronalen Netzen. 
    TensorFlow Lite wurde speziell entwickelt, um TensorFlow-Modelle für ressourcenbeschränkte Geräte zu optimieren und diese auf Embedded-Systemen lauffähig zu machen.
    
    \item \textbf{scikit-learn}: Dieses Framework bietet eine umfassende Bibliothek für klassische Machine-Learning-Algorithmen wie 
    Entscheidungsbäume, Random Forests, Support Vector Machines (SVMs), lineare Modelle, K-Means und mehr. Diese Modelle sind oft weniger rechenintensiv 
    und daher für viele industrielle Anwendungen gut geeignet.

    \item \textbf{XGBoost}: XGBoost ist bekannt für seine Effizienz und Leistung bei Gradient-Boosting-Modellen und eignet sich gut für industrielle Anwendungen 
    wie Anomalieerkennung, Qualitätskontrolle und vorausschauende Wartung.

    \item \textbf{LightGBM}: LightGBM bietet eine ähnliche Funktionalität wie XGBoost, ist jedoch für größere Datensätze und schnellere Trainingszeiten optimiert. 
    Es wird häufig in Echtzeitanwendungen verwendet.

    \item \textbf{PyTorch}: PyTorch ist eine flexible Alternative zu TensorFlow, insbesondere für Forschungsprojekte und Produktionssysteme, 
    die dynamische Berechnungsgrafen erfordern. Es wird auch in Embedded-ML-Anwendungen verwendet, wenn größere Modelle benötigt werden.

    \item \textbf{CatBoost}: Ein Gradient-Boosting-Framework, das speziell für den Umgang mit kategorischen Daten entwickelt wurde. 
    CatBoost eignet sich besonders gut für industrielle Anwendungen, bei denen viele der Eingabedaten kategorisch sind.

    \item \textbf{ONNX}: ONNX bietet eine plattformübergreifende Möglichkeit, ML-Modelle zu konvertieren und auszuführen, die in verschiedenen Frameworks wie 
    TensorFlow, PyTorch oder scikit-learn entwickelt wurden. ONNX erleichtert das Deployment von Modellen auf Embedded- und Edge-Geräten.
\end{itemize}

\subsection{Rust}
Rust ist eine moderne Systemprogrammiersprache, die besonders für ihre Speicher- und Speichersicherheitsfunktionen bekannt ist. 
Sie wird zunehmend für Embedded- und Edge-Anwendungen verwendet, da sie hohe Leistung und Sicherheit bietet.

\begin{itemize}
    \item \textbf{Rust-ML-Bibliotheken}: Obwohl Rust weniger bekannt für Machine Learning ist als Python, gibt es Bibliotheken wie \textbf{Linfa}, 
    die klassische ML-Algorithmen wie lineare Regression, K-Means und Entscheidungsbäume bieten.

    \item \textbf{Tch-RS (PyTorch in Rust)}: Tch-RS ist eine Rust-Bindung für PyTorch, die es ermöglicht, PyTorch-Modelle in der Rust-Umgebung zu verwenden. 
    Diese Bibliothek bietet Zugriff auf viele der in PyTorch vorhandenen Funktionen und kann in eingebetteten Umgebungen verwendet werden, in denen Rust bevorzugt wird.

    \item \textbf{SmartCore}: Eine weitere Machine-Learning-Bibliothek in Rust, die viele der klassischen ML-Algorithmen wie Entscheidungsbäume, 
    Random Forests und lineare Modelle unterstützt. Sie eignet sich gut für eingebettete Anwendungen, bei denen Performance und Sicherheit eine Rolle spielen.

    \item \textbf{ONNX mit Rust}: ONNX kann auch in Rust integriert werden, um neuronale Netze oder klassische Modelle, die in anderen Frameworks entwickelt wurden, 
    auf Embedded-Systemen bereitzustellen.
\end{itemize}

\section{Tools für Modelloptimierung und Konvertierung}

\subsection{TensorFlow Lite}
TensorFlow Lite ist eine leichtgewichtige Version von TensorFlow, die speziell für den Einsatz auf mobilen und eingebetteten Geräten entwickelt wurde. 
Es unterstützt verschiedene Techniken wie Quantisierung, Pruning und Delegates, um die Modellgröße und Rechenanforderungen zu reduzieren.

\subsection{ONNX Runtime}
ONNX Runtime ermöglicht es, ML-Modelle, die in verschiedenen Frameworks wie TensorFlow, PyTorch oder scikit-learn entwickelt wurden, 
auf einer Vielzahl von Hardwareplattformen auszuführen. Es bietet eine Möglichkeit, Modelle plattformübergreifend zu optimieren und effizient auf 
Edge- und Embedded-Systemen bereitzustellen.

\subsection{Apache TVM}
Apache TVM ist ein Maschinelles Learning Compiler-Framework, das es ermöglicht, ML-Modelle für eine Vielzahl von Hardwarearchitekturen zu optimieren. 
TVM bietet spezielle Optimierungen für Embedded-Geräte und ermöglicht es, Modelle effizient auf Mikrocontrollern, GPUs und Edge Computing Plattformen auszuführen.

\subsection{TinyML Frameworks}
TinyML Frameworks wie TensorFlow Micro und uTensor sind speziell für die Ausführung von ML-Modellen auf extrem ressourcenbeschränkten Geräten wie 
Mikrocontrollern und Sensoren konzipiert. Diese Frameworks ermöglichen es, Modelle auf Kleinstgeräten zu implementieren, ohne die Rechenressourcen zu überlasten.

\subsection{TensorRT}
TensorRT ist ein High-Performance-Deep-Learning-Inferenz-Optimierer, der von NVIDIA entwickelt wurde. Es wird verwendet, um die Inferenzleistung von 
neuronalen Netzen auf NVIDIA-Plattformen wie Jetson Nano zu beschleunigen, und eignet sich besonders für Echtzeitanwendungen, die auf leistungsstarker Edge-Hardware laufen.

\section{Tools für Deployment-Optimierung und Kontrolle}

\subsection{Docker}
Docker wird verwendet, um Machine-Learning-Modelle und deren Abhängigkeiten in Containern zu verpacken, sodass sie auf verschiedenen Plattformen 
bereitgestellt werden können. Dies vereinfacht das Deployment von Modellen in Produktionsumgebungen und ermöglicht eine einfache Skalierbarkeit und Wiederholbarkeit.

\subsection{K3s (leichtgewichtige Kubernetes-Distribution)}
K3s ist eine leichtgewichtige Kubernetes-Distribution, die speziell für Edge- und IoT-Geräte entwickelt wurde. Es ermöglicht die Orchestrierung von 
Machine-Learning-Modellen auf verschiedenen Geräten und die Verwaltung von ML-Diensten in einer Edge-Computing-Umgebung.

\subsection{MLflow}
MLflow wird zur Verwaltung des gesamten Lebenszyklus von Machine-Learning-Modellen eingesetzt. Es ermöglicht die Versionierung, das Tracking und das 
Deployment von Modellen und ist besonders nützlich, wenn verschiedene Modellversionen verwaltet und überwacht werden müssen.

\subsection{PyInstaller}
PyInstaller wird verwendet, um Python-Anwendungen und Machine-Learning-Modelle in ausführbare Dateien zu konvertieren, die auf Geräten ohne 
Python-Laufzeitumgebung ausgeführt werden können. Es ermöglicht das Deployment von ML-Modellen als Windows-Dienste oder eigenständige Anwendungen 
auf IPCs oder Windows-basierten Embedded-Systemen.

\subsection{Zephyr RTOS}
Zephyr ist ein Echtzeitbetriebssystem (RTOS), das speziell für Embedded-Geräte entwickelt wurde. Es bietet eine stabile Plattform für die Ausführung 
von ML-Modellen auf Mikrocontrollern und unterstützt Echtzeitanforderungen, die in der industriellen Fertigung entscheidend sind.

\subsection{Conda und Pip}
\textbf{Conda} und \textbf{Pip} sind essentielle Werkzeuge, um Abhängigkeiten für Machine-Learning-Projekte zu verwalten. Beide Systeme wurden verwendet, 
um die unterschiedlichen Anforderungen des Projekts zu erfüllen:
\begin{itemize}
    \item \textbf{Conda} wurde zur Verwaltung von umfangreichen Abhängigkeiten wie \textit{TensorFlow}, \textit{PyTorch} und \textit{OpenVINO} verwendet. 
    Conda bietet eine effizientere Verwaltung großer Bibliotheken und ermöglicht die Integration von Systembibliotheken mit Python-Paketen. Dadurch wurde 
    sichergestellt, dass das Framework auch auf ressourcenbeschränkten Geräten effizient läuft.
    \item \textbf{Pip} wurde für Bibliotheken eingesetzt, die nicht über Conda verfügbar sind oder spezifische Anforderungen haben, wie z.B. \textit{onnxmltools} 
    oder \textit{tensorflow-model-optimization}.
\end{itemize}

Die Kombination dieser Werkzeuge ermöglichte es, ein Framework zu entwickeln, das ML-Modelle effizient auf Embedded Systems implementiert.

\section{Optimierungstechniken}

Die Optimierung von ML-Modellen für Embedded Systems ist ein kritischer Bestandteil dieses Projekts. Folgende Techniken wurden implementiert:

\subsection{Quantisierung}
Durch Quantisierung wird die Größe eines ML-Modells reduziert, indem die Genauigkeit der Gewichtungen von 32-Bit-Gleitkommazahlen auf 8-Bit-Ganzzahlen verringert wird. 
Dies führt zu einer deutlichen Reduktion des Speicherbedarfs und ermöglicht es, die Modelle auf Geräten mit begrenzten Ressourcen auszuführen. 
Diese Technik wurde insbesondere bei Modellen angewendet, die auf SPS und IPCs mit begrenzter Speicher- und Rechenkapazität eingesetzt werden.

\subsection{Model Pruning}
Beim Model Pruning werden unwichtige Neuronen und Verbindungen aus dem ML-Modell entfernt. Dieser Prozess verringert die Modellgröße, ohne die Genauigkeit signifikant 
zu beeinträchtigen. Durch das Pruning konnten wir die Ausführungsgeschwindigkeit auf Embedded Systems verbessern, während die Genauigkeit nur minimal reduziert wurde.

\subsection{Edge-Optimierte Algorithmen}
Zusätzlich zu den oben genannten Techniken wurden speziell für Embedded Systems optimierte Algorithmen implementiert. Diese Algorithmen sind darauf ausgelegt, 
mit minimalem Rechenaufwand auszukommen und gleichzeitig Echtzeitentscheidungen zu ermöglichen. Ein Beispiel hierfür sind binäre Entscheidungsbäume und lineare Modelle, 
die weniger komplex sind als neuronale Netzwerke, jedoch in vielen industriellen Anwendungen ausreichend leistungsfähig sind.

\section{Implementierungsschritte}

\subsection{Anforderungsanalyse}
Zunächst wurden die Anforderungen der industriellen Partner erfasst und eine Anforderungsanalyse durchgeführt. Die wichtigsten Anforderungen konzentrierten sich auf 
die Reduktion der Modellgröße, die Sicherstellung der Echtzeitfähigkeit sowie die ressourcenschonende Ausführung auf Embedded-Devices. Besondere Beachtung fanden 
dabei die spezifischen Hardware-Einschränkungen der eingesetzten SPS- und IPC-Systeme. Zudem wurden Zielfunktionen und Prioritäten festgelegt, um eine Balance 
zwischen Modellkomplexität, Laufzeitleistung und Speicherverbrauch zu finden.

\subsection{Entwicklung des Frameworks}
Das Framework wurde so entworfen, dass es als flexible Schnittstelle zwischen den optimierten ML-Modellen und den Embedded Systems dient. Die Architektur wurde 
in verschiedene Module unterteilt, die sowohl die Modelloptimierung als auch die Kommunikation zwischen den Geräten erleichtern. Wesentliche Aspekte der 
Entwicklung umfassten:

\begin{itemize}
    \item \textbf{Architekturentwurf}: Die Architektur wurde modular entworfen, um die einfache Integration und den Austausch verschiedener ML-Modelle zu ermöglichen.
    \item \textbf{Modelloptimierungstechniken}: Techniken wie Quantisierung, Pruning und Kompression wurden eingesetzt, um die Modellgröße zu reduzieren und die 
    Ausführungseffizienz zu steigern.
    \item \textbf{API-Design}: Eine flexible API wurde entwickelt, um den Zugriff auf verschiedene ML-Modelle zu ermöglichen und eine einfache 
    Bereitstellung auf verschiedenen Embedded Devices sicherzustellen.
\end{itemize}

\subsection{Integration und Tests}
Das Framework wurde auf verschiedene Embedded Devices, einschließlich SPS und IPCs, integriert und umfassend getestet. Dieser Schritt umfasste:

\begin{itemize}
    \item \textbf{Hardware-Integration}: Die Integration des Frameworks auf unterschiedlichen Embedded-Platformen und die Anpassung an deren spezifische 
    Hardware-Eigenschaften.
    \item \textbf{Unit-Tests}: Unit-Tests wurden verwendet, um die korrekte Funktionalität der wichtigsten Komponenten sicherzustellen.
    \item \textbf{Hardware-in-the-Loop (HIL) Tests}: HIL-Tests wurden durchgeführt, um das Framework unter realen Bedingungen zu testen und sicherzustellen, 
    dass die Modelle in Echtzeit mit der Hardware interagieren.
    \item \textbf{Laufzeitleistungsprüfung}: Es wurden spezifische Benchmarks zur Überprüfung der Ausführungszeiten definiert, um sicherzustellen, 
    dass die Echtzeitanforderungen der industriellen Anwendungen erfüllt werden.
\end{itemize}

\subsection{Evaluation der Ergebnisse}
Nach der Integration wurde das Framework anhand vordefinierter Leistungsmetriken evaluiert:

\begin{itemize}
    \item \textbf{Leistungsmetriken}: Die wichtigsten Metriken umfassen die Ausführungszeit der Modelle, den Speicherverbrauch und den Energieverbrauch 
    auf den Embedded Devices.
    \item \textbf{Vergleich mit den Anforderungen}: Die Ergebnisse wurden mit den in der Anforderungsanalyse definierten Zielwerten verglichen, um zu überprüfen, 
    inwieweit die Anforderungen erfüllt wurden.
    \item \textbf{Robustheit und Fehlerbehandlung}: Es wurden Tests durchgeführt, um die Robustheit des Frameworks und seine Fähigkeit, auf Fehler wie 
    Hardwareausfälle oder Modellfehler zu reagieren, zu bewerten.
    \item \textbf{Optimierungspotential}: Basierend auf den Evaluierungsergebnissen wurde Potenzial für weitere Optimierungen, wie die Reduktion des 
    Energieverbrauchs oder die Verbesserung der Ausführungszeiten, identifiziert.
\end{itemize}

\subsection{Zusätzliche Anforderungen}
Weitere Themen die zu beachten sind:

\begin{itemize}
    \item \textbf{Sicherheit und Datenschutz}: Es wurde darauf geachtet, dass das Framework den Anforderungen an die Datensicherheit in der industriellen 
    Fertigung entspricht.
    \item \textbf{Flexibilität und Erweiterbarkeit}: Das Framework wurde so gestaltet, dass zukünftige Erweiterungen und neue ML-Modelle einfach integriert werden können.
    \item \textbf{Wartung und Updates}: Mechanismen für einfache Wartung und Modell-Updates wurden implementiert, um die langfristige Verwendbarkeit 
    des Frameworks zu gewährleisten.
\end{itemize}

\section{Zusammenfassung}
Das entwickelte Framework wurde basierend auf einer detaillierten Anforderungsanalyse erstellt, modular entworfen und für die Ausführung auf 
Embedded Devices optimiert. Durch umfassende Tests und Evaluationen konnte die Effizienz, Echtzeitfähigkeit und Robustheit des Frameworks sichergestellt werden. 
Optimierungsmöglichkeiten wurden identifiziert und bilden die Grundlage für zukünftige Verbesserungen.