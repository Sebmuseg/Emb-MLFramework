\chapter{Evaluation}
\label{chap:evaluation}

Dieses Kapitel evaluiert die Leistung des entwickelten Frameworks anhand verschiedener Metriken. Die Schwerpunkte liegen auf Latenzzeiten, Durchsatz, Ressourcenauslastung und Modellgenauigkeit. Die Evaluation erfolgt auf verschiedenen Embedded Systemen, um sicherzustellen, dass das Framework die spezifischen Anforderungen der Zielsysteme erfüllt und in Echtzeitanwendungen sowohl robust als auch effizient arbeitet.

Ein zentraler Aspekt der Evaluation ist die Reproduzierbarkeit der Tests. Um die statistische Aussagekraft und die Konsistenz der Ergebnisse zu gewährleisten, werden alle Tests mehrfach unter identischen Bedingungen durchgeführt.

\section{Zielsetzung der Evaluation}

Die Evaluation verfolgt das Ziel, die Effektivität und Eignung des Frameworks durch den Vergleich verschiedener Methodiken nachzuweisen. Die Hauptfragestellungen umfassen:
\begin{itemize}
\item Erfüllt das Framework die Echtzeitanforderungen industrieller Anwendungen?
\item Wie verhalten sich Latenz und Durchsatz der Modelle auf unterschiedlichen Embedded Systemen?
\item In welchem Umfang verringern die angewendeten Optimierungstechniken die Modellgröße?
\item Wie hoch ist die Auslastung der Ressourcen (CPU, Speicher) während der Modellausführung?
\item Wie robust ist das Framework gegenüber variierenden Eingabebedingungen?
\end{itemize}

\section{Testumgebung und Reproduzierbarkeit der Tests}

Die Tests erfolgen auf verschiedenen Embedded Systemen, die den typischen Anforderungen industrieller Anwendungen entsprechen. Die Auswahl dieser Systeme ermöglicht eine spezifische Bewertung der Eignung des Frameworks für unterschiedliche Hardwareplattformen und die Bestimmung der optimalen Lösung für Echtzeitanwendungen.

Um die Reproduzierbarkeit und Zuverlässigkeit der Testergebnisse sicherzustellen, wurden alle Tests unter standardisierten Bedingungen und mehrfach wiederholt. Jede Testreihe umfasst mindestens zehn Durchläufe pro Hardwarekonfiguration und Evaluierungsmetrik, um statistisch signifikante und verlässliche Ergebnisse zu gewährleisten. Zusätzlich wurden Variationen der Eingabedaten für jede Testreihe berücksichtigt, um die Robustheit und Konsistenz des Frameworks unter verschiedenen Szenarien zu evaluieren.

Die folgenden Plattformen wurden für die Evaluation verwendet:

\begin{description}
\item[Speicherprogrammierbare Steuerungen (SPS):] Die Tests auf SPS wurden zehnmal unter identischen Bedingungen wiederholt, um die Reproduzierbarkeit der Echtzeitleistung zu validieren. Dabei wurde die Fähigkeit des Frameworks überprüft, strenge Latenzanforderungen in einer stabilen und reproduzierbaren Weise einzuhalten.
\item[] \item[Industrie-PCs (IPCs):] Auf IPCs wurden sowohl einfache als auch komplexere ML-Modelle getestet. Jede Konfiguration wurde zehnmal ausgeführt, um die Reproduzierbarkeit der Ergebnisse und die Konstanz der Leistung zu gewährleisten. Diese Wiederholungen erlauben es, den Einfluss von Modellgröße und Rechenaufwand auf die Effizienz und Echtzeitfähigkeit zu beurteilen.

\item[Mikrocontroller:] Die Tests auf Mikrocontrollern erfolgten ebenfalls in zehn Durchläufen, um die Auswirkungen der Ressourcenbeschränkungen auf die Modellleistung konsistent zu bewerten. Der Fokus lag hier auf der Überprüfung der Effizienz der eingesetzten Optimierungstechniken, wie Quantisierung und Pruning.

\item[Edge-Devices:] Die Evaluation auf leistungsstarken Edge-Geräten umfasste zehn Testwiederholungen, um die Stabilität der Datenverarbeitung und das Verhalten der Modelle bei größeren Datenmengen zu analysieren. Hiermit wird gewährleistet, dass die Ergebnisse unabhängig von Schwankungen im Rechenaufwand reproduzierbar sind.
\end{description}

Durch den Einsatz dieser verschiedenen Hardwareplattformen und die wiederholte Durchführung der Tests wird ein umfassender Vergleich der Leistung und Effizienz des Frameworks ermöglicht. Die konsistente Wiederholung der Tests dient dazu, potenzielle Schwankungen zu minimieren und die Ergebnisse auf eine solide, reproduzierbare Grundlage zu stellen.

\section{Bewertete Metriken}

Die Leistung des Frameworks wird anhand mehrerer Metriken bewertet, um sicherzustellen, dass die optimierten ML-Modelle die Anforderungen an Echtzeitfähigkeit, Durchsatz und Ressourcennutzung erfüllen. Die Auswahl der Metriken basiert auf ihrer Relevanz für industrielle Echtzeitanwendungen und ermöglicht eine umfassende Analyse der Effizienz und Stabilität des Systems. Um die statistische Aussagekraft zu erhöhen, wurden die Metriken in jeder Testreihe wiederholt gemessen und die Ergebnisse in Mittelwerten und Varianzen angegeben.

\begin{description}
\item[Latenz und Echtzeitfähigkeit:] Die Latenz der ML-Modelle wird in jedem Durchlauf gemessen, um die Einhaltung der Echtzeitanforderungen zu überprüfen und die Reproduzierbarkeit der Ergebnisse sicherzustellen. Die folgenden Latenzmetriken werden bewertet:
\begin{itemize}
\item \textbf{Durchschnittliche Latenz pro Vorhersage}: Zehn Messungen pro Modell und Plattform wurden durchgeführt, um die Stabilität der durchschnittlichen Latenz sicherzustellen.
\item \textbf{Maximale Latenz}: Die maximale Vorhersagezeit wird über alle Durchläufe hinweg aufgezeichnet, um sicherzustellen, dass die Echtzeitanforderungen unter sämtlichen Bedingungen eingehalten werden.
\item \textbf{Jitter}: Die Latenzschwankung (Jitter) wird berechnet, um die Konsistenz der Modellausführung zu reflektieren. Durch die wiederholten Messungen können Abweichungen präzise dokumentiert und analysiert werden.
\end{itemize}
\item[Durchsatz:] Der Durchsatz (Vorhersagen pro Sekunde) wurde über die zehn Durchläufe für jedes Modell und jede Hardwareplattform gemessen. Diese Metrik ist besonders in Szenarien von Bedeutung, in denen große Datenmengen in kurzer Zeit analysiert werden müssen:
\begin{itemize}
    \item \textbf{Vorhersagen pro Sekunde (Throughput)}: Die Wiederholung der Messungen erlaubt eine präzise Bewertung des maximalen Durchsatzes unter verschiedenen Lastbedingungen.
    \item \textbf{Maximale Datenrate}: Die höchste Menge an Eingabedaten, die das System ohne Verzögerungen verarbeiten kann, wurde über alle Testwiederholungen hinweg gemessen, um eine zuverlässige Leistung zu gewährleisten.
\end{itemize}

\item[Ressourcenauslastung:] Die CPU- und Speicherauslastung wird in jedem Testlauf überwacht, um die Effizienz des Frameworks auf Geräten mit begrenzten Ressourcen zu evaluieren. Durch die Messungen über mehrere Durchläufe hinweg wird sichergestellt, dass die Ressourcennutzung stabil und vorhersehbar ist.
\begin{itemize}
    \item \textbf{CPU-Auslastung}: Die CPU-Auslastung wurde über die zehn Durchläufe hinweg gemessen, um konsistente Leistung und Effizienz sicherzustellen.
    \item \textbf{Speicherverbrauch}: Der Speicherverbrauch des Frameworks und der ausgeführten Modelle wurde ebenfalls über alle Durchläufe dokumentiert, um die Eignung für ressourcenbeschränkte Umgebungen zu bewerten.
\end{itemize}
\end{description}

\subsection{Modellgenauigkeit und Robustheit}

Die Genauigkeit der optimierten Modelle wird evaluiert, um sicherzustellen, dass die angewendeten Optimierungstechniken die Modellleistung nicht signifikant beeinträchtigen. Durch wiederholte Messungen der Modellgenauigkeit wird die Reproduzierbarkeit der Ergebnisse sichergestellt. Zudem wird die Robustheit der Modelle gegenüber variierenden Eingabebedingungen überprüft.

\begin{description}
\item[Modellgenauigkeit vor und nach der Optimierung:] Die Genauigkeit wird sowohl vor als auch nach der Anwendung von Optimierungstechniken (Quantisierung und Pruning) gemessen. Jede Konfiguration wird zehnmal getestet, um die Konsistenz der Ergebnisse zu gewährleisten.
\item[Robustheit gegenüber verrauschten Eingaben:] Durch Variationen der Eingabebedingungen in zehn Testreihen wird die Fähigkeit des Modells überprüft, auch bei verrauschten oder fehlerhaften Daten konsistente Vorhersagen zu treffen.
\end{description}

\section{Ergebnisse und Diskussion}

\begin{description}
\item[Latenz und Durchsatz:] Die Tests zeigen, dass das Framework die Echtzeitanforderungen erfüllt und reproduzierbare Leistung liefert. Die Latenz bleibt in allen Testreihen unterhalb der vorgegebenen Grenze, und der Durchsatz entspricht den Anforderungen industrieller Anwendungen, was eine effiziente Datenverarbeitung ermöglicht.
\item[Ressourcenauslastung:] Die Analyse der CPU- und Speicherauslastung zeigt, dass das Framework die Ressourcen der verschiedenen Embedded-Geräte optimal nutzt und durch die angewendeten Optimierungstechniken eine signifikante Reduktion des Speicherverbrauchs erreicht. Die wiederholten Messungen bestätigen die Stabilität der Ressourcennutzung auf allen Plattformen.

\item[Modellgenauigkeit und Robustheit:] Die Genauigkeit bleibt nach der Optimierung konsistent und nahezu unverändert. Tests mit verrauschten Eingaben belegen, dass das Framework robust gegenüber variierenden Eingabebedingungen ist und in den meisten Fällen reproduzierbare und verlässliche Vorhersagen liefert.
\item[] \end{description}

\section{Zusammenfassung der Evaluation}

Die wiederholten Tests und die daraus abgeleiteten Ergebnisse zeigen, dass das Framework den festgelegten Anforderungen an Latenz, Durchsatz, Ressourcenauslastung und Modellgenauigkeit entspricht. Die Reproduzierbarkeit der Tests durch zehnfache Wiederholungen pro Szenario gewährleistet die Zuverlässigkeit und statistische Validität der erzielten Ergebnisse. Die konsistente Leistungsfähigkeit über mehrere Testreihen hinweg bestätigt die Robustheit und Effizienz des Frameworks und stellt sicher, dass es die Anforderungen industrieller Echtzeitanwendungen erfüllt.

Diese Ergänzung legt besonderen Wert auf die Reproduzierbarkeit und die Häufigkeit der Testwiederholungen, um die Zuverlässigkeit der Evaluierungsergebnisse zu gewährleisten.