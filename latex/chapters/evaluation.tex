\chapter{Evaluation}
\label{chap:evaluation}

Dieses Kapitel evaluiert die Leistung des entwickelten Frameworks anhand verschiedener Metriken. 
Die Schwerpunkte liegen auf Latenzzeiten, Durchsatz, Ressourcenauslastung und Modellgenauigkeit. 
Die Evaluation erfolgt auf verschiedenen Embedded Systemen, um sicherzustellen, dass das Framework 
die spezifischen Anforderungen der Zielsysteme erfüllt und in Echtzeitanwendungen sowohl robust als auch effizient arbeitet.

\section{Zielsetzung der Evaluation}

Die Evaluation verfolgt das Ziel, die Effektivität und Eignung des Frameworks durch den Vergleich verschiedener 
Methodiken nachzuweisen. Die Hauptfragestellungen umfassen:
\begin{itemize}
    \item Erfüllt das Framework die Echtzeitanforderungen industrieller Anwendungen?
    \item Wie verhalten sich Latenz und Durchsatz der Modelle auf unterschiedlichen Embedded Systemen?
    \item In welchem Umfang verringern die angewendeten Optimierungstechniken die Modellgröße?
    \item Wie hoch ist die Auslastung der Ressourcen (CPU, Speicher) während der Modellausführung?
    \item Wie robust ist das Framework gegenüber variierenden Eingabebedingungen?
\end{itemize}

\section{Testumgebung und Reproduzierbarkeit der Tests}

Die Tests erfolgen auf verschiedenen Embedded Systemen, die den typischen Anforderungen industrieller Anwendungen entsprechen. 
Die Auswahl dieser Systeme ermöglicht eine spezifische Bewertung der Eignung des Frameworks für unterschiedliche Hardwareplattformen 
und die Bestimmung der optimalen Lösung für Echtzeitanwendungen. Um die Reproduzierbarkeit zu gewährleisten, wurden alle Tests 
mehrfach unter identischen Bedingungen durchgeführt. Jede Testreihe umfasst mindestens fünf Durchläufe, um statistisch signifikante 
und zuverlässige Ergebnisse zu erzielen.

Zusätzlich zu physischen Geräten wird die Testumgebung durch Docker-Container und virtualisierte SPS-Plattformen, 
wie beispielsweise Nexeed von Bosch Connected Industry, erweitert. Diese virtuellen Systeme bieten die Möglichkeit, 
das Framework unabhängig von der spezifischen Hardwareumgebung zu testen und zu validieren. 
Durch diese Flexibilität kann das Framework auf eine breitere Palette von Plattformen übertragen werden, 
ohne dass eine spezielle physische Testhardware erforderlich ist. Virtuelle SPS ermöglichen es zudem, 
verschiedene Hard- und Softwarekombinationen zu simulieren und die Tests sowohl konsistent als auch skalierbar zu wiederholen.

\begin{description}
    \item[Speicherprogrammierbare Steuerungen (SPS):] SPS bieten eine stabile Plattform zur Bewertung der Echtzeitleistung in industriellen Steuerungsumgebungen. 
    Sie ermöglichen eine Überprüfung der Fähigkeit des Frameworks, strenge Latenzanforderungen einzuhalten. Zusätzlich zu den physischen SPS 
    wurde eine virtualisierte SPS-Umgebung (z. B. mit Nexeed) genutzt, um die Testbarkeit und Reproduzierbarkeit auf verschiedenen Systemen zu gewährleisten.    
    \item[Industrie-PCs (IPCs):] Die Evaluation auf IPCs erlaubt eine detaillierte Analyse der Leistung komplexerer Modelle, die eine höhere Rechenleistung erfordern. 
    IPCs bieten eine größere Rechenkapazität und ermöglichen die Ausführung anspruchsvoller Datenverarbeitungsaufgaben.
    
    \item[Mikrocontroller:] Mikrocontroller stellen eine ressourcenbeschränkte Umgebung dar, in der die Effizienz der Optimierungstechniken geprüft wird. 
    Hier werden insbesondere Techniken wie Quantisierung und Pruning unter den Bedingungen eingeschränkter Ressourcen getestet.
    
    \item[Edge-Devices:] Leistungsstarke Edge-Geräte dienen der Vorverarbeitung und dem Training der Modelle. Sie ermöglichen eine Beurteilung der Eignung des 
    Frameworks für Aufgaben mit höherem Rechenbedarf und größerem Datenvolumen, bevor die Modelle auf den Embedded Systemen bereitgestellt werden.
\end{description}

Der Einsatz von verschiedenen Hardwareplattformen in Kombination mit Docker-Containern und virtuellen SPS-Lösungen ermöglicht eine umfassende und skalierbare Testumgebung. 
Dies gewährleistet, dass das Framework unter den spezifischen Anforderungen jeder Plattform eine zuverlässige Echtzeitausführung sicherstellt und die gewählten Methodiken 
reproduzierbare und valide Ergebnisse liefern. Die Verwendung von Containern und virtuellen SPS-Umgebungen vereinfacht zudem die zukünftige Validierung auf weiteren Plattformen 
und bietet eine flexible Grundlage für die kontinuierliche Optimierung und Erweiterung des Frameworks.
\section{Bewertete Metriken}

Die Leistung des Frameworks wird anhand mehrerer Metriken bewertet, um sicherzustellen, dass die optimierten ML-Modelle die 
Anforderungen an Echtzeitfähigkeit, Durchsatz und Ressourcennutzung erfüllen. Die Auswahl der Metriken basiert auf der Relevanz 
für industrielle Echtzeitanwendungen und ermöglicht eine umfassende Analyse der Effizienz und Stabilität des Systems.

\begin{description}
    \item[Latenz und Echtzeitfähigkeit:] Die Latenz der ML-Modelle wird gemessen, um die Einhaltung der Echtzeitanforderungen 
    zu überprüfen. Die Tests erfolgen in verschiedenen Szenarien, um die folgenden Aspekte zu bewerten:
    \begin{itemize}
        \item \textbf{Durchschnittliche Latenz pro Vorhersage}: Die mittlere Zeit, die das Modell für eine Vorhersage benötigt, 
        um eine stabile Echtzeitausführung zu gewährleisten.
        \item \textbf{Maximale Latenz}: Die höchste gemessene Zeit für eine Vorhersage, um sicherzustellen, dass die Echtzeitanforderungen 
        unter sämtlichen Bedingungen eingehalten werden.
        \item \textbf{Jitter}: Die Schwankung der Latenz bei mehreren Vorhersagen unter denselben Bedingungen, 
        was die Konsistenz der Modellausführung reflektiert.
    \end{itemize}

    \item[Durchsatz:] Der Durchsatz bewertet die Anzahl an Vorhersagen, die das System pro Sekunde verarbeiten kann. 
    Diese Metrik ist besonders in Szenarien von Bedeutung, in denen große Datenmengen in kurzer Zeit analysiert werden müssen:
    \begin{itemize}
        \item \textbf{Vorhersagen pro Sekunde (Throughput)}: Die Anzahl an Vorhersagen, die das System pro Sekunde durchführt, 
        um eine effiziente Datenverarbeitung sicherzustellen.
        \item \textbf{Maximale Datenrate}: Die höchste Menge an Eingabedaten, die das System ohne Verzögerungen verarbeiten kann, 
        um eine zuverlässige Leistung zu gewährleisten.
    \end{itemize}

    \item[Ressourcenauslastung:] Die CPU- und Speicherauslastung wird während der Modellausführung überwacht, um die Stabilität und 
    Effizienz des Systems auch bei eingeschränkten Ressourcen zu gewährleisten.
    \begin{itemize}
        \item \textbf{CPU-Auslastung}: Der Anteil der CPU-Ressourcen, die während der Modellausführung verwendet werden, 
        um die Effizienz des Frameworks auf Geräten mit begrenzter Leistung sicherzustellen.
        \item \textbf{Speicherverbrauch}: Der statische und dynamische Speicherverbrauch des Frameworks und der ausgeführten Modelle, 
        um die Eignung für ressourcenbeschränkte Umgebungen zu evaluieren.
    \end{itemize}
\end{description}

\subsection{Modellgenauigkeit und Robustheit}

Die Genauigkeit der optimierten Modelle wird evaluiert, um sicherzustellen, dass die angewendeten Optimierungstechniken die Modellleistung 
nicht signifikant beeinträchtigen. Zusätzlich wird die Robustheit der Modelle gegenüber fehlerhaften oder verrauschten Eingabedaten geprüft. 

\begin{description}
    \item[Modellgenauigkeit vor und nach der Optimierung:] Die Vorhersagegenauigkeit des Modells wird sowohl vor als auch 
    nach der Anwendung von Optimierungstechniken wie Quantisierung und Pruning gemessen, um deren Auswirkungen auf die Leistung zu quantifizieren.
    \item[Robustheit gegenüber verrauschten Eingaben:] Die Fähigkeit des Modells, unter verrauschten oder fehlerhaften 
    Eingabedaten konsistente Vorhersagen zu treffen, wird getestet, um die Zuverlässigkeit in realen Szenarien sicherzustellen.
\end{description}

\section{Ergebnisse und Diskussion}

\begin{description}
    \item[Latenz und Durchsatz:] Die Tests zeigen, dass das Framework in der Lage ist, die vorgegebenen Echtzeitanforderungen zu erfüllen. 
    Die Latenz bleibt in den meisten Tests unterhalb der vorgegebenen Grenze, und der Durchsatz entspricht den Anforderungen industrieller Anwendungen, 
    was eine effiziente Datenverarbeitung ermöglicht.
    
    \item[Ressourcenauslastung:] Die Analyse der CPU- und Speicherauslastung zeigt, dass das Framework effizient arbeitet und die Ressourcen 
    der verschiedenen Embedded-Geräte optimal nutzt. Besonders durch die Optimierungstechniken wird der Speicherverbrauch signifikant reduziert, 
    wodurch das Framework auf Geräten mit begrenztem Speicher stabil läuft.
    
    \item[Modellgenauigkeit und Robustheit:] Die Genauigkeit der Modelle bleibt nach der Optimierung nahezu unverändert. 
    Tests mit verrauschten Eingaben belegen, dass das Framework robust gegenüber variierenden Eingabebedingungen 
    ist und in den meisten Fällen konsistente und verlässliche Vorhersagen liefert.
\end{description}

\section{Zusammenfassung der Evaluation}

Die durchgeführten Tests und die daraus abgeleiteten Ergebnisse zeigen, dass das Framework den festgelegten Anforderungen an Latenz, 
Durchsatz, Ressourcenauslastung und Modellgenauigkeit entspricht. Das Framework ermöglicht eine effiziente und robuste 
Vorhersage auf verschiedenen Embedded-Systemen und erfüllt somit die Anforderungen für industrielle Echtzeitanwendungen. 
Die Vergleichsanalyse der angewendeten Methodiken bestätigt deren Eignung als optimale Lösung zur Umsetzung 
der gesteckten Leistungsziele. Die Reproduzierbarkeit der Tests durch mehrfache Wiederholungen gewährleistet die 
Zuverlässigkeit und statistische Validität der erzielten Ergebnisse.