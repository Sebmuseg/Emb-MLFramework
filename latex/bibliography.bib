@inbook{inbook,
author = {Ünlü, Ramazan and Söylemez, İsmet},
year = {2024},
month = {09},
pages = {207-233},
title = {AI-Driven Predictive Maintenance},
isbn = {978-981-97-5978-1},
doi = {10.1007/978-981-97-5979-8_10}
}

@phdthesis{phdthesis,
author = {Yesilbas, Dilara},
year = {2022},
month = {01},
pages = {},
title = {Deep Learning in Automatic Optical Inspection - Selected Studies in the Context of an Automotive Supplier}
}

@article{article,
author = {Park, Jungme and Aryal, Pawan and Mandumula, Sai and Asolkar, Ritwik},
year = {2023},
month = {04},
pages = {3992},
title = {An Optimized DNN Model for Real-Time Inferencing on an Embedded Device},
volume = {23},
journal = {Sensors},
doi = {10.3390/s23083992}
}

@Article{s23042131,
AUTHOR = {Biglari, Amin and Tang, Wei},
TITLE = {A Review of Embedded Machine Learning Based on Hardware, Application, and Sensing Scheme},
JOURNAL = {Sensors},
VOLUME = {23},
YEAR = {2023},
NUMBER = {4},
ARTICLE-NUMBER = {2131},
URL = {https://www.mdpi.com/1424-8220/23/4/2131},
PubMedID = {36850729},
ISSN = {1424-8220},
ABSTRACT = {Machine learning is an expanding field with an ever-increasing role in everyday life, with its utility in the industrial, agricultural, and medical sectors being undeniable. Recently, this utility has come in the form of machine learning implementation on embedded system devices. While there have been steady advances in the performance, memory, and power consumption of embedded devices, most machine learning algorithms still have a very high power consumption and computational demand, making the implementation of embedded machine learning somewhat difficult. However, different devices can be implemented for different applications based on their overall processing power and performance. This paper presents an overview of several different implementations of machine learning on embedded systems divided by their specific device, application, specific machine learning algorithm, and sensors. We will mainly focus on NVIDIA Jetson and Raspberry Pi devices with a few different less utilized embedded computers, as well as which of these devices were more commonly used for specific applications in different fields. We will also briefly analyze the specific ML models most commonly implemented on the devices and the specific sensors that were used to gather input from the field. All of the papers included in this review were selected using Google Scholar and published papers in the IEEExplore database. The selection criterion for these papers was the usage of embedded computing systems in either a theoretical study or practical implementation of machine learning models. The papers needed to have provided either one or, preferably, all of the following results in their studies—the overall accuracy of the models on the system, the overall power consumption of the embedded machine learning system, and the inference time of their models on the embedded system. Embedded machine learning is experiencing an explosion in both scale and scope, both due to advances in system performance and machine learning models, as well as greater affordability and accessibility of both. Improvements are noted in quality, power usage, and effectiveness.},
DOI = {10.3390/s23042131}
}

@INPROCEEDINGS{10212294,
  author={Satyam and Geetha, P. and Shashikala, K.S. and Kumar, N. Ashok},
  booktitle={2023 2nd International Conference on Edge Computing and Applications (ICECAA)}, 
  title={AI-Enabled Edge Computing Models: Trends, Developments, and Future Implications}, 
  year={2023},
  volume={},
  number={},
  pages={63-67},
  abstract={This research study examines the ways in which AI-enabled edge computing (AIEC) models are being used across various industries to improve operational efficiency and enable faster decision-making. The study discusses the key trends and developments of shaping the future of these next generation computing models, including the increased adoption of EC, the growing use of IoT devices, the emergence of 5G networks, advancements in machine learning algorithms, and the focus on data security. The research study also highlights case studies from different industries to illustrate the benefits of AIEC models and the potential impacts on businesses. The findings of this research work suggests that these computing models have the potential to transform the way organizations operate, leading to increased efficiency and productivity, better customer experiences, and more informed decision-making.},
  keywords={Industries;Machine learning algorithms;Computational modeling;Decision making;Organizations;Transforms;Market research;Artificial Intelligence;Edge computing;Internet of Things;Machine Learning;5G;Data security;Industry 4.0},
  doi={10.1109/ICECAA58104.2023.10212294},
  ISSN={},
  month={July},
}

@article{haigh2015machine,
  title={Machine learning for embedded systems: A case study},
  author={Haigh, Karen Zita and Mackay, Allan M and Cook, Michael R and Lin, Li G},
  journal={BBN Technologies: Cambridge, MA, USA},
  volume={8571},
  pages={1--12},
  year={2015}
}

@article{farbiz2023knowledge,
  title={Knowledge-embedded machine learning and its applications in smart manufacturing},
  author={Farbiz, Farzam and Habibullah, Mohd Salahuddin and Hamadicharef, Brahim and Maszczyk, Tomasz and Aggarwal, Saurabh},
  journal={Journal of Intelligent Manufacturing},
  volume={34},
  number={7},
  pages={2889--2906},
  year={2023},
  publisher={Springer}
}

@article{nadeski2019bringing,
  title={Bringing machine learning to embedded systems},
  author={Nadeski, Mark},
  journal={Texas Instruments},
  pages={1--5},
  year={2019}
}

@ARTICLE{10087221,
  author={Larrakoetxea, Nerea Gómez and Astobiza, Joseba Eskubi and López, Iker Pastor and Urquijo, Borja Sanz and Barruetabeńa, Jon García and Rego, Agustin Zubillaga},
  journal={IEEE Access}, 
  title={Efficient Machine Learning on Edge Computing Through Data Compression Techniques}, 
  year={2023},
  volume={11},
  number={},
  pages={31676-31685},
  keywords={Bayes methods;Data models;Machine learning;Decoding;Machine learning algorithms;Computational modeling;Edge computing;Autoencoder;Bayesian network;big data;edge computing;machine learning},
  doi={10.1109/ACCESS.2023.3263391}}

@book{Soldatos2024,
  title={Artificial Intelligence in Manufacturing: Enabling Intelligent, Flexible and Cost-Effective Production Through AI},
  editor={John Soldatos},
  year={2024},
  publisher={Springer Cham},
  doi={10.1007/978-3-031-46452-2},
  isbn={978-3-031-46452-2},
  url={https://doi.org/10.1007/978-3-031-46452-2},
  edition={1},
  pages={XXVII, 505}
}

@INPROCEEDINGS{8119409,
  author={Yao, Xifan and Zhou, Jiajun and Zhang, Jiangming and Boër, Claudio R.},
  booktitle={2017 5th International Conference on Enterprise Systems (ES)}, 
  title={From Intelligent Manufacturing to Smart Manufacturing for Industry 4.0 Driven by Next Generation Artificial Intelligence and Further On}, 
  year={2017},
  volume={},
  number={},
  pages={311-318},
  keywords={Big Data;Expert systems;Industries;Artificial neural networks;Artificial intelligence;intelligent manufacturing;smart manufacturing;cyber-physical system;social-cyber-physical system;big data;Industry 4.0},
  doi={10.1109/ES.2017.58}}


@Article{telecom4010011,
AUTHOR = {Podder, Itilekha and Fischl, Tamas and Bub, Udo},
TITLE = {Artificial Intelligence Applications for MEMS-Based Sensors and Manufacturing Process Optimization},
JOURNAL = {Telecom},
VOLUME = {4},
YEAR = {2023},
NUMBER = {1},
PAGES = {165--197},
URL = {https://www.mdpi.com/2673-4001/4/1/11},
ISSN = {2673-4001},
ABSTRACT = {Micro-electromechanical systems (MEMS) technology-based sensors have found diverse fields of application due to the advancement in semiconductor manufacturing technology, which produces sensitive, low-cost, and powerful sensors. Due to the fabrication of different electrical and mechanical components on a single chip and complex process steps, MEMS sensors are prone to deterministic and random errors. Thus, testing, calibration, and quality control have become obligatory to maintain the quality and reliability of the sensors. This is where Artificial Intelligence (AI) can provide significant benefits, such as handling complex data, performing root cause analysis, efficient feature estimation, process optimization, product improvement, time-saving, automation, fault diagnosis and detection, drift compensation, signal de-noising, etc. Despite several benefits, the embodiment of AI poses multiple challenges. This review paper provides a systematic, in-depth analysis of AI applications in the MEMS-based sensors field for both the product and the system level adaptability by analyzing more than 100 articles. This paper summarizes the state-of-the-art, current trends of AI applications in MEMS sensors and outlines the challenges of AI incorporation in an industrial setting to improve manufacturing processes. Finally, we reflect upon all the findings based on the three proposed research questions to discover the future research scope.},
DOI = {10.3390/telecom4010011}
}

@article{articleQuantization,
author = {Zhou, Yiren and Moosavi-Dezfooli, Seyed-Mohsen and Cheung, Ngai-Man and Frossard, Pascal},
year = {2017},
month = {12},
pages = {},
title = {Adaptive Quantization for Deep Neural Network},
volume = {32},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v32i1.11623}
}

-------------------

@ARTICLE{8478188,
  author={Illa, Prasanna Kumar and Padhi, Nikhil},
  journal={IEEE Access}, 
  title={Practical Guide to Smart Factory Transition Using IoT, Big Data and Edge Analytics}, 
  year={2018},
  volume={6},
  number={},
  pages={55162-55170},
  keywords={Sensors;Smart manufacturing;Production facilities;Computer architecture;Real-time systems;Big Data;Big data;cloud;edge computing;data integration;Internet of Things;PaaS;Smart factory},
  doi={10.1109/ACCESS.2018.2872799}}

@ARTICLE{9380634,
  author={Merluzzi, Mattia and Lorenzo, Paolo Di and Barbarossa, Sergio},
  journal={IEEE Access}, 
  title={Wireless Edge Machine Learning: Resource Allocation and Trade-Offs}, 
  year={2021},
  volume={9},
  number={},
  pages={45377-45398},
  keywords={Delays;Task analysis;Servers;Resource management;Reliability;Heuristic algorithms;Machine learning;Edge machine learning;multi-access edge computing;computation offloading;stochastic optimization;resource allocation;energy-latency-accuracy trade-off},
  doi={10.1109/ACCESS.2021.3066559}}


@ARTICLE{9893787,
  author={Zaidi, Syed Ali Raza and Hayajneh, Ali M. and Hafeez, Maryam and Ahmed, Q. Z.},
  journal={IEEE Access}, 
  title={Unlocking Edge Intelligence Through Tiny Machine Learning (TinyML)}, 
  year={2022},
  volume={10},
  number={},
  pages={100867-100877},
  keywords={Internet of Things;Logic gates;Cloud computing;Performance evaluation;Computational modeling;Memory management;Machine learning;Edge computing;Internet of Things;Deep learning;Transfer learning;Collaborative work;Energy efficiency;Tiny machine learning;IoT;edge computing;5G;LoRa;gesture recognition;deep learning;transfer learning;federated learning;implementation;MLOps;energy efficiency},
  doi={10.1109/ACCESS.2022.3207200}}

@ARTICLE{9911615,
  author={Liu, Tianyi and Sugano, Yusuke},
  journal={IEEE Access}, 
  title={Interactive Machine Learning on Edge Devices With User-in-the-Loop Sample Recommendation}, 
  year={2022},
  volume={10},
  number={},
  pages={107346-107360},
  abstract={Interactive machine learning (IML) aims to make machine learning an easy-to-use tool for novice users to solve personalized tasks. However, despite the recent popularity of edge AI, research into interactive machine learning on edge devices has not been conducted actively. Existing IML designs cannot be directly applied to small edge devices due to interface and computational resource limitations. In this paper, we propose a method for efficient model personalization on a small interactive object recognition camera device by combining sample recommendations with an IML workflow. The proposed method recommends training data candidates from unlabeled samples in addition to the usual annotation operations. Our method interactively trains a noise filter to handle a noisy sample pool obtained while using the device. The user can indicate whether the recommended sample corresponds to 1) the recommended class; 2) other classes; or 3) noise unrelated to the recognition task by providing ternary feedback. Our system is designed to gradually update both the target classifier and the noise filtering recommendation modules on the basis of feedback. We show that our feedback design achieves more efficient model training while improving system usability through a systematic evaluation and user study using a prototype device.},
  keywords={Machine learning;Computer vision;Annotations;Data models;Computational modeling;User interfaces;Graphical user interfaces;Edge computing;Interactive systems;Computer vision;edge machine learning;interactive machine learning;user interface},
  doi={10.1109/ACCESS.2022.3212077},
  ISSN={2169-3536},
  month={},}

@ARTICLE{10433185,
  author={Capogrosso, Luigi and Cunico, Federico and Cheng, Dong Seon and Fummi, Franco and Cristani, Marco},
  journal={IEEE Access}, 
  title={A Machine Learning-Oriented Survey on Tiny Machine Learning}, 
  year={2024},
  volume={12},
  number={},
  pages={23406-23426},
  abstract={The emergence of Tiny Machine Learning (TinyML) has positively revolutionized the field of Artificial Intelligence by promoting the joint design of resource-constrained IoT hardware devices and their learning-based software architectures. TinyML carries an essential role within the fourth and fifth industrial revolutions in helping societies, economies, and individuals employ effective AI-infused computing technologies (e.g., smart cities, automotive, and medical robotics). Given its multidisciplinary nature, the field of TinyML has been approached from many different angles: this comprehensive survey wishes to provide an up-to-date overview focused on all the learning algorithms within TinyML-based solutions. The survey is based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodological flow, allowing for a systematic and complete literature survey. In particular, firstly, we will examine the three different workflows for implementing a TinyML-based system, i.e., ML-oriented, HW-oriented, and co-design. Secondly, we propose a taxonomy that covers the learning panorama under the TinyML lens, examining in detail the different families of model optimization and design, as well as the state-of-the-art learning techniques. Thirdly, this survey will present the distinct features of hardware devices and software tools that represent the current state-of-the-art for TinyML intelligent edge applications. Finally, we discuss the challenges and future directions.},
  keywords={Surveys;Hardware;Systematics;Benchmark testing;Computational modeling;Anomaly detection;Training;Deep learning;Embedded systems;Edge computing;Machine learning;Artificial intelligence;Internet of Things;Micromachining;TinyML;edge intelligence;efficient deep learning;embedded systems},
  doi={10.1109/ACCESS.2024.3365349},
  ISSN={2169-3536},
  month={},}

@ARTICLE{9985008,
  author={Shuvo, Md. Maruf Hossain and Islam, Syed Kamrul and Cheng, Jianlin and Morshed, Bashir I.},
  journal={Proceedings of the IEEE}, 
  title={Efficient Acceleration of Deep Learning Inference on Resource-Constrained Edge Devices: A Review}, 
  year={2023},
  volume={111},
  number={1},
  pages={42-91},
  abstract={Successful integration of deep neural networks (DNNs) or deep learning (DL) has resulted in breakthroughs in many areas. However, deploying these highly accurate models for data-driven, learned, automatic, and practical machine learning (ML) solutions to end-user applications remains challenging. DL algorithms are often computationally expensive, power-hungry, and require large memory to process complex and iterative operations of millions of parameters. Hence, training and inference of DL models are typically performed on high-performance computing (HPC) clusters in the cloud. Data transmission to the cloud results in high latency, round-trip delay, security and privacy concerns, and the inability of real-time decisions. Thus, processing on edge devices can significantly reduce cloud transmission cost. Edge devices are end devices closest to the user, such as mobile phones, cyber–physical systems (CPSs), wearables, the Internet of Things (IoT), embedded and autonomous systems, and intelligent sensors. These devices have limited memory, computing resources, and power-handling capability. Therefore, optimization techniques at both the hardware and software levels have been developed to handle the DL deployment efficiently on the edge. Understanding the existing research, challenges, and opportunities is fundamental to leveraging the next generation of edge devices with artificial intelligence (AI) capability. Mainly, four research directions have been pursued for efficient DL inference on edge devices: 1) novel DL architecture and algorithm design; 2) optimization of existing DL methods; 3) development of algorithm–hardware codesign; and 4) efficient accelerator design for DL deployment. This article focuses on surveying each of the four research directions, providing a comprehensive review of the state-of-the-art tools and techniques for efficient edge inference.},
  keywords={Edge computing;Image edge detection;Real-time systems;Cloud computing;Artificial intelligence;Optimization;Computer architecture;Neural networks;Deep learning;Algorithm–hardware codesign;artificial intelligence (AI);artificial intelligence on edge (edge-AI);deep learning (DL);model compression;neural accelerator},
  doi={10.1109/JPROC.2022.3226481},
  ISSN={1558-2256},
  month={Jan},}

@ARTICLE{9893137,
  author={Diab, Maha S. and Rodriguez-Villegas, Esther},
  journal={IEEE Access}, 
  title={Embedded Machine Learning Using Microcontrollers in Wearable and Ambulatory Systems for Health and Care Applications: A Review}, 
  year={2022},
  volume={10},
  number={},
  pages={98450-98474},
  abstract={The use of machine learning in medical and assistive applications is receiving significant attention thanks to the unique potential it offers to solve complex healthcare problems for which no other solutions had been found. Particularly promising in this field is the combination of machine learning with novel wearable devices. Machine learning models, however, suffer from being computationally demanding, which typically has resulted on the acquired data having to be transmitted to remote cloud servers for inference. This is not ideal from the system’s requirements point of view. Recently, efforts to replace the cloud servers with an alternative inference device closer to the sensing platform, has given rise to a new area of research Tiny Machine Learning (TinyML). In this work, we investigate the different challenges and specifications trade-offs associated to existing hardware options, as well as recently developed software tools, when trying to use microcontroller units (MCUs) as inference devices for health and care applications. The paper also reviews existing wearable systems incorporating MCUs for monitoring, and management, in the context of different health and care intended uses. Overall, this work addresses the gap in literature targeting the use of MCUs as edge inference devices for healthcare wearables. Thus, can be used as a kick-start for embedding machine learning models on MCUs, focusing on healthcare wearables.},
  keywords={Medical services;Machine learning;Wearable computers;Power demand;Machine learning algorithms;Wearable computers;Microcontrollers;Edge ML;embedded machine learning;healthcare;microcontroller;TinyML;wearable},
  doi={10.1109/ACCESS.2022.3206782},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{10150033,
  author={Ringler, Nicolas and Knittel, Dominique and Ponsart, Jean-Christophe and Nouari, Mohammed and Yakob, Abderrahmane and Romani, Daniel},
  booktitle={2023 IEEE IAS Global Conference on Emerging Technologies (GlobConET)}, 
  title={Machine Learning based Real Time Predictive Maintenance at the Edge for Manufacturing Systems: A Practical Example}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Edge Computing is becoming more and more essential for the Industrial Internet of Things (industrial IoT) for predictive maintenance in the industry 4.0 framework. The transition from central to distributed approaches (edge nodes), will enhance the capabilities of handling real-time big data from IoT by ensuring low latency and high bandwidth. Moreover, with the developed architecture the possibility is given to have distributed Machine Learning (ML) by enabling edge devices to learn local ML models. Therefore, the performances of the global diagnostic models can be improved. In this paper, the developed setup is composed of PC’s, NVIDIA Jetson Nano Developers kits (for edge computing), and a smartphone for real-time displaying. The implemented real-time supervised machine learning approaches are applied on an industrial oil-injection screw compressor instrumented with vibration sensors. Time domain features are calculated online with the help of sliding windows and the features are automatically classified. Embedded in the equipment, the used algorithms obtained very good real-time diagnostic performances.},
  keywords={Machine learning algorithms;Machine learning;Computer architecture;Fasteners;Prediction algorithms;Real-time systems;Sensors;predictive maintenance;diagnostic;supervised machine learning;time series;embedded edge computing},
  doi={10.1109/GlobConET56651.2023.10150033},
  ISSN={},
  month={May},}

@ARTICLE{10373847,
  author={Wong, Junhua and Nerbonne, Jeanne and Zhang, Qingxue},
  journal={IEEE Access}, 
  title={Ultra-Efficient Edge Cardiac Disease Detection Towards Real-Time Precision Health}, 
  year={2024},
  volume={12},
  number={},
  pages={9940-9951},
  abstract={Nowadays, intensive interests are targeting the deep learning on edge precision health towards instantaneous disease measurements. However, edge inference usually has constrained computing resource, which poses a great challenge on running the heavy deep learning for real-time measurements. In this study, we propose to leverage a knowledge distillation methodology to enable ultra-efficient deep learning on edge. We take a special interest in Electrocardiogram (ECG)-based cardiac abnormality measurement. More specifically, we propose to train two deep learning models, including a heavy teacher model and a light-weight student model, and leverage the ‘soft target distribution’ learned by the teacher model to supervise the learning of the student model. So, the powerful teacher model can transfer learned knowledge to the student model and boost the latter’s accuracy. Further, to mitigate the vulnerability of the deep learning model under adversarial attacks, we further introduce preserving-robustness learning to the student model, without needing extra computing resources, through enhancing its loss function under adversarial perturbations. Our experiments on real-time heart disease measurement have demonstrated that, the learned lightweight student model, with a model reduction of 45x and under adversarial attacks, can still achieve comparable disease detection performance. The proposed robust knowledge distillation methodology has effectively enabled light-weight and secure cardiac measurement. Significance: This study is expected to contribute to on-edge deep learning-powered disease detection, for real-time, long term, and secured cardiac precision health.},
  keywords={Deep learning;Biological system modeling;Electrocardiography;Image edge detection;Biomedical measurement;Real-time systems;Heart;Cardiac disease;Cardiology;Deep learning;edge inference;cardiac disease;real-time measurement},
  doi={10.1109/ACCESS.2023.3346893},
  ISSN={2169-3536},
  month={},}

@ARTICLE{10537163,
  author={Amin, Rashed Al and Hasan, Mehrab and Wiese, Veit and Obermaisser, Roman},
  journal={IEEE Access}, 
  title={FPGA-Based Real-Time Object Detection and Classification System Using YOLO for Edge Computing}, 
  year={2024},
  volume={12},
  number={},
  pages={73268-73278},
  abstract={The leap forward in research progress in real-time object detection and classification has been dramatically boosted by including Embedded Artificial Intelligence (EAI) and Deep Learning (DL). Real-time object detection and classification with deep learning require many resources and computational power, which makes it more difficult to use deep learning methods on edge devices. This paper proposed a new, highly efficient Field Programmable Gate Array (FPGA) based real-time object detection and classification system using You Only Look Once (YOLO) v3 Tiny for edge computing. However, the proposed system has been instantiated with Advanced Driving Assistance Systems (ADAS) for evaluation. Traffic light detection and classification are crucial in ADAS to ensure drivers’ safety. The proposed system used a camera connected to the Kria KV260 FPGA development board to detect and classify the traffic light. Bosch Small Traffic Light Dataset (BSTLD) has been used to train the YOLO model, and Xilinx Vitis AI has been used to quantify and compile the YOLO model. The proposed system can detect and classify traffic light signals from a high-definition (HD) video streaming in 15 frames per second (FPS) with 99% accuracy. In addition, it consumes only 3.5W power, demonstrating the ability to work on edge devices. The on-road experimental results represent fast, precise, and reliable detection and classification of traffic lights in the proposed system. Overall, this paper demonstrates a low-cost and highly efficient FPGA-based system for real-time object detection and classification.},
  keywords={YOLO;Field programmable gate arrays;Real-time systems;Power demand;Deep learning;Edge computing;Throughput;Object detection;Classification algorithms;FPGAs;object detection and classification;YOLO;edge computing},
  doi={10.1109/ACCESS.2024.3404623},
  ISSN={2169-3536},
  month={},}

@ARTICLE{10343149,
  author={Takele, Atallo Kassaw and Villányi, Balázs},
  journal={IEEE Access}, 
  title={LSTM-Autoencoder-Based Incremental Learning for Industrial Internet of Things}, 
  year={2023},
  volume={11},
  number={},
  pages={137929-137936},
  abstract={Edge-based intelligent data analytics supports the Industrial Internet of Things (IIoT) to enable efficient manufacturing. Incremental learning in the edge-based data analytics has the potential to analyze continuously collected real-time data. However, additional efforts are needed to address performance, latency, resource utilization and storage of historical data challenges. This paper introduces an incremental learning approach based on Long-Short Term Memory (LSTM) autoencoders, by sparsening the weight matrix and taking samples from previously trained sub-datasets. The aim is to minimize the resources utilized while training redundant knowledge for edge devices of IIoT. The degree of sparsity can be determined by the redundancy of patterns, and the inverse of the coefficient of variation has been utilized to recognize it. A higher value of the inverse of the coefficient of variation shows that the values of the weight matrix are close to each other, which indicates the redundancy of knowledge, and vice versa. In addition, the coefficient of variation has been applied for limiting the size of samples from the previously trained sub-datasets. The experiment conducted using the IIoT testbed dataset demonstrates substantial enhancements in resource optimization without compromising performance.},
  keywords={Training;Industrial Internet of Things;Real-time systems;Performance evaluation;Edge computing;Task analysis;Servers;Encoding;Weight measurement;Industrial internet of things (IIoT);incremental learning;LSTM-autoencoder;weight sparsification},
  doi={10.1109/ACCESS.2023.3339556},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{9838829,
  author={Zhang, Liang and Jabbari, Bijan},
  booktitle={ICC 2022 - IEEE International Conference on Communications}, 
  title={Machine Learning Driven Latency Optimization for Application-aware Edge Computing-based IoTs}, 
  year={2022},
  volume={},
  number={},
  pages={183-188},
  abstract={Most IoT devices have limited or no computing capability while many emerging IoT applications require both computing and communications services. Moreover, low latency requirements of numerous applications such as autonomous driving and augmented reality are becoming critical. In this paper, we propose a novel framework that can utilize the edge-computing facilities and the full-duplex technique at the edge nodes to address computing and communication services with low latency to IoT terminals for different applications. We then formulate an application-aware edge-computing problem for IoTs with the target to minimize the average latency. We propose a machine learning algorithm to solve this problem by achieving the best user-edge-node assignment and developing an optimal assignment and scheduling algorithm for the communication and computing resources. We evaluate the performance of the proposed machine learning algorithm (via Python and Tensor ow) and present results and comparison with other methods.},
  keywords={Machine learning algorithms;Tensors;Scheduling algorithms;Conferences;Machine learning;Full-duplex system;Delays;Internet of Things (IoT);latency;IoT applications;optimization;machine learning (ML);artificial intelligence (AI);5G;6G},
  doi={10.1109/ICC45855.2022.9838829},
  ISSN={1938-1883},
  month={May},}

@ARTICLE{9427249,
  author={Zhou, Ian and Makhdoom, Imran and Shariati, Negin and Raza, Muhammad Ahmad and Keshavarz, Rasool and Lipman, Justin and Abolhasan, Mehran and Jamalipour, Abbas},
  journal={IEEE Access}, 
  title={Internet of Things 2.0: Concepts, Applications, and Future Directions}, 
  year={2021},
  volume={9},
  number={},
  pages={70961-71012},
  abstract={Applications and technologies of the Internet of Things are in high demand with the increase of network devices. With the development of technologies such as 5G, machine learning, edge computing, and Industry 4.0, the Internet of Things has evolved. This survey article discusses the evolution of the Internet of Things and presents the vision for Internet of Things 2.0. The Internet of Things 2.0 development is discussed across seven major fields. These fields are machine learning intelligence, mission critical communication, scalability, energy harvesting-based energy sustainability, interoperability, user friendly IoT, and security. Other than these major fields, the architectural development of the Internet of Things and major types of applications are also reviewed. Finally, this article ends with the vision and current limitations of the Internet of Things in future network environments.},
  keywords={Internet of Things;5G mobile communication;Edge computing;Computer architecture;Tactile Internet;Security;Machine learning;IoT;IoT2.0;machine learning;mission critical communication;scalability;energy harvesting;interoperability;usability;security;5G;6G},
  doi={10.1109/ACCESS.2021.3078549},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{8342870,
  author={Kanawaday, Ameeth and Sane, Aditya},
  booktitle={2017 8th IEEE International Conference on Software Engineering and Service Science (ICSESS)}, 
  title={Machine learning for predictive maintenance of industrial machines using IoT sensor data}, 
  year={2017},
  volume={},
  number={},
  pages={87-90},
  abstract={The industrial Internet of Things (IIoT) is the use of Internet of Things (IoT) technologies in manufacturing which harnesses the machine data generated by various sensors and applies various analytics on it to gain useful information. The data captured by the machines is usually accompanied by a date time component which proves vital for predictive modelling. This paper explores the use of AutoRegressive Integrated Moving Average (ARIMA) forecasting on the time series data collected from various sensors from a Slitting Machine, to predict the possible failures and quality defects, thus improving the overall manufacturing process. The use of Machine Learning thus proves a vital component in IIoT having use cases in quality management and quality control, lowering the cost of maintenance and improving the overall manufacturing process.},
  keywords={Predictive models;Production;Data models;Machine learning;Predictive maintenance;Forecasting;Machine Learning;ARIMA Forecasting;Data Analysis;Predictive maintenance & Productivity},
  doi={10.1109/ICSESS.2017.8342870},
  ISSN={2327-0594},
  month={Nov},}

@INPROCEEDINGS{8387672,
  author={Westbrink, Fabian and Chadha, Gavneet Singh and Schwung, Andreas},
  booktitle={2018 IEEE Industrial Cyber-Physical Systems (ICPS)}, 
  title={Integrated IPC for data-driven fault detection}, 
  year={2018},
  volume={},
  number={},
  pages={277-282},
  abstract={Condition monitoring and fault detection are of critical importance in modern industrial processes for efficient and productive machine operation. Early and accurate detection of faults facilitates in the efficient and secure operation of the plant by preventing losses due to machinery breakdown. In this study, we present a novel and rapid fault detection methodology using an integrated Industrial PC. The approach is based on integrating existing hardware components and software libraries for efficient application of machine learning algorithms to an industrial process. The Industrial PC is integrated within the fieldbus system of a Bulk Good Laboratory Plant providing superior network security as compared to an external connection to a cloud platform. Additionally, the Industrial PC provides predominantly better numerical computation functionality as compared to traditional PLC environment due to the availability of machine learning libraries in high-level languages. The fault detection procedure is accomplished with the Principal Component Analysis dimensionality reduction algorithm along with Hotelling's T2 statistic. A variety of sensor fault cases pertinent to the Bulk Good Laboratory Plant are analysed and experimental results show that all fault cases were detected with low detection delays. The time required to detect faults reflects the real-time capabilities of the system in an industrial scenario.},
  keywords={Fault detection;Principal component analysis;Cloud computing;Libraries;Monitoring;Hardware;Machine learning algorithms;Integrated Industrial PC;Fault Detection;Principal Component Analysis;Bulk Good Laboratory Plant},
  doi={10.1109/ICPHYS.2018.8387672},
  ISSN={},
  month={May},}

@INPROCEEDINGS{10398108,
  author={Rai, Anjani Kumar and Pokhariya, Hemant Singh and Tiwari, Kripanshu and Divya Vani, V and Kumar, Devesh and Kumar, Arjun and Rana, Ajay},
  booktitle={2023 6th International Conference on Contemporary Computing and Informatics (IC3I)}, 
  title={IOT Driven Predictive Maintenance Using Machine Learning Algorithms}, 
  year={2023},
  volume={6},
  number={},
  pages={2674-2678},
  abstract={The modern Web of Things (IIoT) alludes to the use of Web of Things (IoT) innovation underway that empowers the use of machine information created by different sensors and uses different investigation on it to get adroit information. The data typically accompany a date and time when they are caught by the gadgets. fundamental part for predictive demonstrating. The development of the Web of Things (IoT), which gives continuous information from sensors and associated gadgets, has totally changed the modern scene. Predictive support has turned into a progressive strategy for working on the efficiency and reliability of fundamental machinery and framework here. To empower IoT-driven predictive upkeep plans, machine learning algorithms assume a basic part, which is succinctly summed up in this theoretical. To anticipate hardware disappointments and identify upkeep needs before they bring about costly breakdowns, IoT-driven predictive support utilizes the consistent stream of information from sensors and gadgets. With their ability to break down huge measures of information and recognize complex examples, machine learning algorithms have become significant to this venture.},
  keywords={Technological innovation;Machine learning algorithms;Costs;Electric breakdown;Hardware;Sensors;Task analysis;IOT-Driven;Predictive;Machine Learning;Algorithms},
  doi={10.1109/IC3I59117.2023.10398108},
  ISSN={},
  month={Sep.},}

@article{Shi2020,
  author    = {Shi, Stone},
  title     = {{INTERNATIONAL: Edge computing, artificial intelligence, automation innovation: Edge computing can help engineers lower costs and develop new applications, such as shop-floor data analysis and quality prediction. Artificial intelligence (AI) improves efficiency and accuracy}},
  journal   = {Control Engineering},
  volume    = {67},
  number    = {11},
  pages     = {8},
  month     = nov,
  year      = {2020},
  url       = {https://link.gale.com/apps/doc/A649927625/AONE?u=anon~77372379&sid=googleScholar&xid=a90843a5},
  note      = {Accessed 28 Oct. 2024}
}

