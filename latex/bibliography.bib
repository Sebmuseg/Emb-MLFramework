@inbook{inbook,
author = {Ünlü, Ramazan and Söylemez, İsmet},
year = {2024},
month = {09},
pages = {207-233},
title = {AI-Driven Predictive Maintenance},
isbn = {978-981-97-5978-1},
doi = {10.1007/978-981-97-5979-8_10}
}

@phdthesis{phdthesis,
author = {Yesilbas, Dilara},
year = {2022},
month = {01},
pages = {},
title = {Deep Learning in Automatic Optical Inspection - Selected Studies in the Context of an Automotive Supplier}
}

@article{article,
author = {Park, Jungme and Aryal, Pawan and Mandumula, Sai and Asolkar, Ritwik},
year = {2023},
month = {04},
pages = {3992},
title = {An Optimized DNN Model for Real-Time Inferencing on an Embedded Device},
volume = {23},
journal = {Sensors},
doi = {10.3390/s23083992}
}

@Article{s23042131,
AUTHOR = {Biglari, Amin and Tang, Wei},
TITLE = {A Review of Embedded Machine Learning Based on Hardware, Application, and Sensing Scheme},
JOURNAL = {Sensors},
VOLUME = {23},
YEAR = {2023},
NUMBER = {4},
ARTICLE-NUMBER = {2131},
URL = {https://www.mdpi.com/1424-8220/23/4/2131},
PubMedID = {36850729},
ISSN = {1424-8220},
ABSTRACT = {Machine learning is an expanding field with an ever-increasing role in everyday life, with its utility in the industrial, agricultural, and medical sectors being undeniable. Recently, this utility has come in the form of machine learning implementation on embedded system devices. While there have been steady advances in the performance, memory, and power consumption of embedded devices, most machine learning algorithms still have a very high power consumption and computational demand, making the implementation of embedded machine learning somewhat difficult. However, different devices can be implemented for different applications based on their overall processing power and performance. This paper presents an overview of several different implementations of machine learning on embedded systems divided by their specific device, application, specific machine learning algorithm, and sensors. We will mainly focus on NVIDIA Jetson and Raspberry Pi devices with a few different less utilized embedded computers, as well as which of these devices were more commonly used for specific applications in different fields. We will also briefly analyze the specific ML models most commonly implemented on the devices and the specific sensors that were used to gather input from the field. All of the papers included in this review were selected using Google Scholar and published papers in the IEEExplore database. The selection criterion for these papers was the usage of embedded computing systems in either a theoretical study or practical implementation of machine learning models. The papers needed to have provided either one or, preferably, all of the following results in their studies—the overall accuracy of the models on the system, the overall power consumption of the embedded machine learning system, and the inference time of their models on the embedded system. Embedded machine learning is experiencing an explosion in both scale and scope, both due to advances in system performance and machine learning models, as well as greater affordability and accessibility of both. Improvements are noted in quality, power usage, and effectiveness.},
DOI = {10.3390/s23042131}
}

@INPROCEEDINGS{10212294,
  author={Satyam and Geetha, P. and Shashikala, K.S. and Kumar, N. Ashok},
  booktitle={2023 2nd International Conference on Edge Computing and Applications (ICECAA)}, 
  title={AI-Enabled Edge Computing Models: Trends, Developments, and Future Implications}, 
  year={2023},
  volume={},
  number={},
  pages={63-67},
  abstract={This research study examines the ways in which AI-enabled edge computing (AIEC) models are being used across various industries to improve operational efficiency and enable faster decision-making. The study discusses the key trends and developments of shaping the future of these next generation computing models, including the increased adoption of EC, the growing use of IoT devices, the emergence of 5G networks, advancements in machine learning algorithms, and the focus on data security. The research study also highlights case studies from different industries to illustrate the benefits of AIEC models and the potential impacts on businesses. The findings of this research work suggests that these computing models have the potential to transform the way organizations operate, leading to increased efficiency and productivity, better customer experiences, and more informed decision-making.},
  keywords={Industries;Machine learning algorithms;Computational modeling;Decision making;Organizations;Transforms;Market research;Artificial Intelligence;Edge computing;Internet of Things;Machine Learning;5G;Data security;Industry 4.0},
  doi={10.1109/ICECAA58104.2023.10212294},
  ISSN={},
  month={July},
}

@article{haigh2015machine,
  title={Machine learning for embedded systems: A case study},
  author={Haigh, Karen Zita and Mackay, Allan M and Cook, Michael R and Lin, Li G},
  journal={BBN Technologies: Cambridge, MA, USA},
  volume={8571},
  pages={1--12},
  year={2015}
}

@article{farbiz2023knowledge,
  title={Knowledge-embedded machine learning and its applications in smart manufacturing},
  author={Farbiz, Farzam and Habibullah, Mohd Salahuddin and Hamadicharef, Brahim and Maszczyk, Tomasz and Aggarwal, Saurabh},
  journal={Journal of Intelligent Manufacturing},
  volume={34},
  number={7},
  pages={2889--2906},
  year={2023},
  publisher={Springer}
}

@article{nadeski2019bringing,
  title={Bringing machine learning to embedded systems},
  author={Nadeski, Mark},
  journal={Texas Instruments},
  pages={1--5},
  year={2019}
}

@ARTICLE{10087221,
  author={Larrakoetxea, Nerea Gómez and Astobiza, Joseba Eskubi and López, Iker Pastor and Urquijo, Borja Sanz and Barruetabeńa, Jon García and Rego, Agustin Zubillaga},
  journal={IEEE Access}, 
  title={Efficient Machine Learning on Edge Computing Through Data Compression Techniques}, 
  year={2023},
  volume={11},
  number={},
  pages={31676-31685},
  keywords={Bayes methods;Data models;Machine learning;Decoding;Machine learning algorithms;Computational modeling;Edge computing;Autoencoder;Bayesian network;big data;edge computing;machine learning},
  doi={10.1109/ACCESS.2023.3263391}}

@book{Soldatos2024,
  title={Artificial Intelligence in Manufacturing: Enabling Intelligent, Flexible and Cost-Effective Production Through AI},
  editor={John Soldatos},
  year={2024},
  publisher={Springer Cham},
  doi={10.1007/978-3-031-46452-2},
  isbn={978-3-031-46452-2},
  url={https://doi.org/10.1007/978-3-031-46452-2},
  edition={1},
  pages={XXVII, 505}
}

@INPROCEEDINGS{8119409,
  author={Yao, Xifan and Zhou, Jiajun and Zhang, Jiangming and Boër, Claudio R.},
  booktitle={2017 5th International Conference on Enterprise Systems (ES)}, 
  title={From Intelligent Manufacturing to Smart Manufacturing for Industry 4.0 Driven by Next Generation Artificial Intelligence and Further On}, 
  year={2017},
  volume={},
  number={},
  pages={311-318},
  keywords={Big Data;Expert systems;Industries;Artificial neural networks;Artificial intelligence;intelligent manufacturing;smart manufacturing;cyber-physical system;social-cyber-physical system;big data;Industry 4.0},
  doi={10.1109/ES.2017.58}}


@Article{telecom4010011,
AUTHOR = {Podder, Itilekha and Fischl, Tamas and Bub, Udo},
TITLE = {Artificial Intelligence Applications for MEMS-Based Sensors and Manufacturing Process Optimization},
JOURNAL = {Telecom},
VOLUME = {4},
YEAR = {2023},
NUMBER = {1},
PAGES = {165--197},
URL = {https://www.mdpi.com/2673-4001/4/1/11},
ISSN = {2673-4001},
ABSTRACT = {Micro-electromechanical systems (MEMS) technology-based sensors have found diverse fields of application due to the advancement in semiconductor manufacturing technology, which produces sensitive, low-cost, and powerful sensors. Due to the fabrication of different electrical and mechanical components on a single chip and complex process steps, MEMS sensors are prone to deterministic and random errors. Thus, testing, calibration, and quality control have become obligatory to maintain the quality and reliability of the sensors. This is where Artificial Intelligence (AI) can provide significant benefits, such as handling complex data, performing root cause analysis, efficient feature estimation, process optimization, product improvement, time-saving, automation, fault diagnosis and detection, drift compensation, signal de-noising, etc. Despite several benefits, the embodiment of AI poses multiple challenges. This review paper provides a systematic, in-depth analysis of AI applications in the MEMS-based sensors field for both the product and the system level adaptability by analyzing more than 100 articles. This paper summarizes the state-of-the-art, current trends of AI applications in MEMS sensors and outlines the challenges of AI incorporation in an industrial setting to improve manufacturing processes. Finally, we reflect upon all the findings based on the three proposed research questions to discover the future research scope.},
DOI = {10.3390/telecom4010011}
}

@article{articleQuantization,
author = {Zhou, Yiren and Moosavi-Dezfooli, Seyed-Mohsen and Cheung, Ngai-Man and Frossard, Pascal},
year = {2017},
month = {12},
pages = {},
title = {Adaptive Quantization for Deep Neural Network},
volume = {32},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v32i1.11623}
}

-------------------

@ARTICLE{8478188,
  author={Illa, Prasanna Kumar and Padhi, Nikhil},
  journal={IEEE Access}, 
  title={Practical Guide to Smart Factory Transition Using IoT, Big Data and Edge Analytics}, 
  year={2018},
  volume={6},
  number={},
  pages={55162-55170},
  keywords={Sensors;Smart manufacturing;Production facilities;Computer architecture;Real-time systems;Big Data;Big data;cloud;edge computing;data integration;Internet of Things;PaaS;Smart factory},
  doi={10.1109/ACCESS.2018.2872799}}

@ARTICLE{9380634,
  author={Merluzzi, Mattia and Lorenzo, Paolo Di and Barbarossa, Sergio},
  journal={IEEE Access}, 
  title={Wireless Edge Machine Learning: Resource Allocation and Trade-Offs}, 
  year={2021},
  volume={9},
  number={},
  pages={45377-45398},
  keywords={Delays;Task analysis;Servers;Resource management;Reliability;Heuristic algorithms;Machine learning;Edge machine learning;multi-access edge computing;computation offloading;stochastic optimization;resource allocation;energy-latency-accuracy trade-off},
  doi={10.1109/ACCESS.2021.3066559}}


@ARTICLE{9893787,
  author={Zaidi, Syed Ali Raza and Hayajneh, Ali M. and Hafeez, Maryam and Ahmed, Q. Z.},
  journal={IEEE Access}, 
  title={Unlocking Edge Intelligence Through Tiny Machine Learning (TinyML)}, 
  year={2022},
  volume={10},
  number={},
  pages={100867-100877},
  keywords={Internet of Things;Logic gates;Cloud computing;Performance evaluation;Computational modeling;Memory management;Machine learning;Edge computing;Internet of Things;Deep learning;Transfer learning;Collaborative work;Energy efficiency;Tiny machine learning;IoT;edge computing;5G;LoRa;gesture recognition;deep learning;transfer learning;federated learning;implementation;MLOps;energy efficiency},
  doi={10.1109/ACCESS.2022.3207200}}

@ARTICLE{9911615,
  author={Liu, Tianyi and Sugano, Yusuke},
  journal={IEEE Access}, 
  title={Interactive Machine Learning on Edge Devices With User-in-the-Loop Sample Recommendation}, 
  year={2022},
  volume={10},
  number={},
  pages={107346-107360},
  abstract={Interactive machine learning (IML) aims to make machine learning an easy-to-use tool for novice users to solve personalized tasks. However, despite the recent popularity of edge AI, research into interactive machine learning on edge devices has not been conducted actively. Existing IML designs cannot be directly applied to small edge devices due to interface and computational resource limitations. In this paper, we propose a method for efficient model personalization on a small interactive object recognition camera device by combining sample recommendations with an IML workflow. The proposed method recommends training data candidates from unlabeled samples in addition to the usual annotation operations. Our method interactively trains a noise filter to handle a noisy sample pool obtained while using the device. The user can indicate whether the recommended sample corresponds to 1) the recommended class; 2) other classes; or 3) noise unrelated to the recognition task by providing ternary feedback. Our system is designed to gradually update both the target classifier and the noise filtering recommendation modules on the basis of feedback. We show that our feedback design achieves more efficient model training while improving system usability through a systematic evaluation and user study using a prototype device.},
  keywords={Machine learning;Computer vision;Annotations;Data models;Computational modeling;User interfaces;Graphical user interfaces;Edge computing;Interactive systems;Computer vision;edge machine learning;interactive machine learning;user interface},
  doi={10.1109/ACCESS.2022.3212077},
  ISSN={2169-3536},
  month={},}

@ARTICLE{10433185,
  author={Capogrosso, Luigi and Cunico, Federico and Cheng, Dong Seon and Fummi, Franco and Cristani, Marco},
  journal={IEEE Access}, 
  title={A Machine Learning-Oriented Survey on Tiny Machine Learning}, 
  year={2024},
  volume={12},
  number={},
  pages={23406-23426},
  abstract={The emergence of Tiny Machine Learning (TinyML) has positively revolutionized the field of Artificial Intelligence by promoting the joint design of resource-constrained IoT hardware devices and their learning-based software architectures. TinyML carries an essential role within the fourth and fifth industrial revolutions in helping societies, economies, and individuals employ effective AI-infused computing technologies (e.g., smart cities, automotive, and medical robotics). Given its multidisciplinary nature, the field of TinyML has been approached from many different angles: this comprehensive survey wishes to provide an up-to-date overview focused on all the learning algorithms within TinyML-based solutions. The survey is based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodological flow, allowing for a systematic and complete literature survey. In particular, firstly, we will examine the three different workflows for implementing a TinyML-based system, i.e., ML-oriented, HW-oriented, and co-design. Secondly, we propose a taxonomy that covers the learning panorama under the TinyML lens, examining in detail the different families of model optimization and design, as well as the state-of-the-art learning techniques. Thirdly, this survey will present the distinct features of hardware devices and software tools that represent the current state-of-the-art for TinyML intelligent edge applications. Finally, we discuss the challenges and future directions.},
  keywords={Surveys;Hardware;Systematics;Benchmark testing;Computational modeling;Anomaly detection;Training;Deep learning;Embedded systems;Edge computing;Machine learning;Artificial intelligence;Internet of Things;Micromachining;TinyML;edge intelligence;efficient deep learning;embedded systems},
  doi={10.1109/ACCESS.2024.3365349},
  ISSN={2169-3536},
  month={},}

@ARTICLE{9985008,
  author={Shuvo, Md. Maruf Hossain and Islam, Syed Kamrul and Cheng, Jianlin and Morshed, Bashir I.},
  journal={Proceedings of the IEEE}, 
  title={Efficient Acceleration of Deep Learning Inference on Resource-Constrained Edge Devices: A Review}, 
  year={2023},
  volume={111},
  number={1},
  pages={42-91},
  abstract={Successful integration of deep neural networks (DNNs) or deep learning (DL) has resulted in breakthroughs in many areas. However, deploying these highly accurate models for data-driven, learned, automatic, and practical machine learning (ML) solutions to end-user applications remains challenging. DL algorithms are often computationally expensive, power-hungry, and require large memory to process complex and iterative operations of millions of parameters. Hence, training and inference of DL models are typically performed on high-performance computing (HPC) clusters in the cloud. Data transmission to the cloud results in high latency, round-trip delay, security and privacy concerns, and the inability of real-time decisions. Thus, processing on edge devices can significantly reduce cloud transmission cost. Edge devices are end devices closest to the user, such as mobile phones, cyber–physical systems (CPSs), wearables, the Internet of Things (IoT), embedded and autonomous systems, and intelligent sensors. These devices have limited memory, computing resources, and power-handling capability. Therefore, optimization techniques at both the hardware and software levels have been developed to handle the DL deployment efficiently on the edge. Understanding the existing research, challenges, and opportunities is fundamental to leveraging the next generation of edge devices with artificial intelligence (AI) capability. Mainly, four research directions have been pursued for efficient DL inference on edge devices: 1) novel DL architecture and algorithm design; 2) optimization of existing DL methods; 3) development of algorithm–hardware codesign; and 4) efficient accelerator design for DL deployment. This article focuses on surveying each of the four research directions, providing a comprehensive review of the state-of-the-art tools and techniques for efficient edge inference.},
  keywords={Edge computing;Image edge detection;Real-time systems;Cloud computing;Artificial intelligence;Optimization;Computer architecture;Neural networks;Deep learning;Algorithm–hardware codesign;artificial intelligence (AI);artificial intelligence on edge (edge-AI);deep learning (DL);model compression;neural accelerator},
  doi={10.1109/JPROC.2022.3226481},
  ISSN={1558-2256},
  month={Jan},}

@ARTICLE{9893137,
  author={Diab, Maha S. and Rodriguez-Villegas, Esther},
  journal={IEEE Access}, 
  title={Embedded Machine Learning Using Microcontrollers in Wearable and Ambulatory Systems for Health and Care Applications: A Review}, 
  year={2022},
  volume={10},
  number={},
  pages={98450-98474},
  abstract={The use of machine learning in medical and assistive applications is receiving significant attention thanks to the unique potential it offers to solve complex healthcare problems for which no other solutions had been found. Particularly promising in this field is the combination of machine learning with novel wearable devices. Machine learning models, however, suffer from being computationally demanding, which typically has resulted on the acquired data having to be transmitted to remote cloud servers for inference. This is not ideal from the system’s requirements point of view. Recently, efforts to replace the cloud servers with an alternative inference device closer to the sensing platform, has given rise to a new area of research Tiny Machine Learning (TinyML). In this work, we investigate the different challenges and specifications trade-offs associated to existing hardware options, as well as recently developed software tools, when trying to use microcontroller units (MCUs) as inference devices for health and care applications. The paper also reviews existing wearable systems incorporating MCUs for monitoring, and management, in the context of different health and care intended uses. Overall, this work addresses the gap in literature targeting the use of MCUs as edge inference devices for healthcare wearables. Thus, can be used as a kick-start for embedding machine learning models on MCUs, focusing on healthcare wearables.},
  keywords={Medical services;Machine learning;Wearable computers;Power demand;Machine learning algorithms;Wearable computers;Microcontrollers;Edge ML;embedded machine learning;healthcare;microcontroller;TinyML;wearable},
  doi={10.1109/ACCESS.2022.3206782},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{10150033,
  author={Ringler, Nicolas and Knittel, Dominique and Ponsart, Jean-Christophe and Nouari, Mohammed and Yakob, Abderrahmane and Romani, Daniel},
  booktitle={2023 IEEE IAS Global Conference on Emerging Technologies (GlobConET)}, 
  title={Machine Learning based Real Time Predictive Maintenance at the Edge for Manufacturing Systems: A Practical Example}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Edge Computing is becoming more and more essential for the Industrial Internet of Things (industrial IoT) for predictive maintenance in the industry 4.0 framework. The transition from central to distributed approaches (edge nodes), will enhance the capabilities of handling real-time big data from IoT by ensuring low latency and high bandwidth. Moreover, with the developed architecture the possibility is given to have distributed Machine Learning (ML) by enabling edge devices to learn local ML models. Therefore, the performances of the global diagnostic models can be improved. In this paper, the developed setup is composed of PC’s, NVIDIA Jetson Nano Developers kits (for edge computing), and a smartphone for real-time displaying. The implemented real-time supervised machine learning approaches are applied on an industrial oil-injection screw compressor instrumented with vibration sensors. Time domain features are calculated online with the help of sliding windows and the features are automatically classified. Embedded in the equipment, the used algorithms obtained very good real-time diagnostic performances.},
  keywords={Machine learning algorithms;Machine learning;Computer architecture;Fasteners;Prediction algorithms;Real-time systems;Sensors;predictive maintenance;diagnostic;supervised machine learning;time series;embedded edge computing},
  doi={10.1109/GlobConET56651.2023.10150033},
  ISSN={},
  month={May},}

@ARTICLE{10373847,
  author={Wong, Junhua and Nerbonne, Jeanne and Zhang, Qingxue},
  journal={IEEE Access}, 
  title={Ultra-Efficient Edge Cardiac Disease Detection Towards Real-Time Precision Health}, 
  year={2024},
  volume={12},
  number={},
  pages={9940-9951},
  abstract={Nowadays, intensive interests are targeting the deep learning on edge precision health towards instantaneous disease measurements. However, edge inference usually has constrained computing resource, which poses a great challenge on running the heavy deep learning for real-time measurements. In this study, we propose to leverage a knowledge distillation methodology to enable ultra-efficient deep learning on edge. We take a special interest in Electrocardiogram (ECG)-based cardiac abnormality measurement. More specifically, we propose to train two deep learning models, including a heavy teacher model and a light-weight student model, and leverage the ‘soft target distribution’ learned by the teacher model to supervise the learning of the student model. So, the powerful teacher model can transfer learned knowledge to the student model and boost the latter’s accuracy. Further, to mitigate the vulnerability of the deep learning model under adversarial attacks, we further introduce preserving-robustness learning to the student model, without needing extra computing resources, through enhancing its loss function under adversarial perturbations. Our experiments on real-time heart disease measurement have demonstrated that, the learned lightweight student model, with a model reduction of 45x and under adversarial attacks, can still achieve comparable disease detection performance. The proposed robust knowledge distillation methodology has effectively enabled light-weight and secure cardiac measurement. Significance: This study is expected to contribute to on-edge deep learning-powered disease detection, for real-time, long term, and secured cardiac precision health.},
  keywords={Deep learning;Biological system modeling;Electrocardiography;Image edge detection;Biomedical measurement;Real-time systems;Heart;Cardiac disease;Cardiology;Deep learning;edge inference;cardiac disease;real-time measurement},
  doi={10.1109/ACCESS.2023.3346893},
  ISSN={2169-3536},
  month={},}

@ARTICLE{10537163,
  author={Amin, Rashed Al and Hasan, Mehrab and Wiese, Veit and Obermaisser, Roman},
  journal={IEEE Access}, 
  title={FPGA-Based Real-Time Object Detection and Classification System Using YOLO for Edge Computing}, 
  year={2024},
  volume={12},
  number={},
  pages={73268-73278},
  abstract={The leap forward in research progress in real-time object detection and classification has been dramatically boosted by including Embedded Artificial Intelligence (EAI) and Deep Learning (DL). Real-time object detection and classification with deep learning require many resources and computational power, which makes it more difficult to use deep learning methods on edge devices. This paper proposed a new, highly efficient Field Programmable Gate Array (FPGA) based real-time object detection and classification system using You Only Look Once (YOLO) v3 Tiny for edge computing. However, the proposed system has been instantiated with Advanced Driving Assistance Systems (ADAS) for evaluation. Traffic light detection and classification are crucial in ADAS to ensure drivers’ safety. The proposed system used a camera connected to the Kria KV260 FPGA development board to detect and classify the traffic light. Bosch Small Traffic Light Dataset (BSTLD) has been used to train the YOLO model, and Xilinx Vitis AI has been used to quantify and compile the YOLO model. The proposed system can detect and classify traffic light signals from a high-definition (HD) video streaming in 15 frames per second (FPS) with 99% accuracy. In addition, it consumes only 3.5W power, demonstrating the ability to work on edge devices. The on-road experimental results represent fast, precise, and reliable detection and classification of traffic lights in the proposed system. Overall, this paper demonstrates a low-cost and highly efficient FPGA-based system for real-time object detection and classification.},
  keywords={YOLO;Field programmable gate arrays;Real-time systems;Power demand;Deep learning;Edge computing;Throughput;Object detection;Classification algorithms;FPGAs;object detection and classification;YOLO;edge computing},
  doi={10.1109/ACCESS.2024.3404623},
  ISSN={2169-3536},
  month={},}

@ARTICLE{10343149,
  author={Takele, Atallo Kassaw and Villányi, Balázs},
  journal={IEEE Access}, 
  title={LSTM-Autoencoder-Based Incremental Learning for Industrial Internet of Things}, 
  year={2023},
  volume={11},
  number={},
  pages={137929-137936},
  abstract={Edge-based intelligent data analytics supports the Industrial Internet of Things (IIoT) to enable efficient manufacturing. Incremental learning in the edge-based data analytics has the potential to analyze continuously collected real-time data. However, additional efforts are needed to address performance, latency, resource utilization and storage of historical data challenges. This paper introduces an incremental learning approach based on Long-Short Term Memory (LSTM) autoencoders, by sparsening the weight matrix and taking samples from previously trained sub-datasets. The aim is to minimize the resources utilized while training redundant knowledge for edge devices of IIoT. The degree of sparsity can be determined by the redundancy of patterns, and the inverse of the coefficient of variation has been utilized to recognize it. A higher value of the inverse of the coefficient of variation shows that the values of the weight matrix are close to each other, which indicates the redundancy of knowledge, and vice versa. In addition, the coefficient of variation has been applied for limiting the size of samples from the previously trained sub-datasets. The experiment conducted using the IIoT testbed dataset demonstrates substantial enhancements in resource optimization without compromising performance.},
  keywords={Training;Industrial Internet of Things;Real-time systems;Performance evaluation;Edge computing;Task analysis;Servers;Encoding;Weight measurement;Industrial internet of things (IIoT);incremental learning;LSTM-autoencoder;weight sparsification},
  doi={10.1109/ACCESS.2023.3339556},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{9838829,
  author={Zhang, Liang and Jabbari, Bijan},
  booktitle={ICC 2022 - IEEE International Conference on Communications}, 
  title={Machine Learning Driven Latency Optimization for Application-aware Edge Computing-based IoTs}, 
  year={2022},
  volume={},
  number={},
  pages={183-188},
  abstract={Most IoT devices have limited or no computing capability while many emerging IoT applications require both computing and communications services. Moreover, low latency requirements of numerous applications such as autonomous driving and augmented reality are becoming critical. In this paper, we propose a novel framework that can utilize the edge-computing facilities and the full-duplex technique at the edge nodes to address computing and communication services with low latency to IoT terminals for different applications. We then formulate an application-aware edge-computing problem for IoTs with the target to minimize the average latency. We propose a machine learning algorithm to solve this problem by achieving the best user-edge-node assignment and developing an optimal assignment and scheduling algorithm for the communication and computing resources. We evaluate the performance of the proposed machine learning algorithm (via Python and Tensor ow) and present results and comparison with other methods.},
  keywords={Machine learning algorithms;Tensors;Scheduling algorithms;Conferences;Machine learning;Full-duplex system;Delays;Internet of Things (IoT);latency;IoT applications;optimization;machine learning (ML);artificial intelligence (AI);5G;6G},
  doi={10.1109/ICC45855.2022.9838829},
  ISSN={1938-1883},
  month={May},}

@ARTICLE{9427249,
  author={Zhou, Ian and Makhdoom, Imran and Shariati, Negin and Raza, Muhammad Ahmad and Keshavarz, Rasool and Lipman, Justin and Abolhasan, Mehran and Jamalipour, Abbas},
  journal={IEEE Access}, 
  title={Internet of Things 2.0: Concepts, Applications, and Future Directions}, 
  year={2021},
  volume={9},
  number={},
  pages={70961-71012},
  abstract={Applications and technologies of the Internet of Things are in high demand with the increase of network devices. With the development of technologies such as 5G, machine learning, edge computing, and Industry 4.0, the Internet of Things has evolved. This survey article discusses the evolution of the Internet of Things and presents the vision for Internet of Things 2.0. The Internet of Things 2.0 development is discussed across seven major fields. These fields are machine learning intelligence, mission critical communication, scalability, energy harvesting-based energy sustainability, interoperability, user friendly IoT, and security. Other than these major fields, the architectural development of the Internet of Things and major types of applications are also reviewed. Finally, this article ends with the vision and current limitations of the Internet of Things in future network environments.},
  keywords={Internet of Things;5G mobile communication;Edge computing;Computer architecture;Tactile Internet;Security;Machine learning;IoT;IoT2.0;machine learning;mission critical communication;scalability;energy harvesting;interoperability;usability;security;5G;6G},
  doi={10.1109/ACCESS.2021.3078549},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{8342870,
  author={Kanawaday, Ameeth and Sane, Aditya},
  booktitle={2017 8th IEEE International Conference on Software Engineering and Service Science (ICSESS)}, 
  title={Machine learning for predictive maintenance of industrial machines using IoT sensor data}, 
  year={2017},
  volume={},
  number={},
  pages={87-90},
  abstract={The industrial Internet of Things (IIoT) is the use of Internet of Things (IoT) technologies in manufacturing which harnesses the machine data generated by various sensors and applies various analytics on it to gain useful information. The data captured by the machines is usually accompanied by a date time component which proves vital for predictive modelling. This paper explores the use of AutoRegressive Integrated Moving Average (ARIMA) forecasting on the time series data collected from various sensors from a Slitting Machine, to predict the possible failures and quality defects, thus improving the overall manufacturing process. The use of Machine Learning thus proves a vital component in IIoT having use cases in quality management and quality control, lowering the cost of maintenance and improving the overall manufacturing process.},
  keywords={Predictive models;Production;Data models;Machine learning;Predictive maintenance;Forecasting;Machine Learning;ARIMA Forecasting;Data Analysis;Predictive maintenance & Productivity},
  doi={10.1109/ICSESS.2017.8342870},
  ISSN={2327-0594},
  month={Nov},}

@INPROCEEDINGS{8387672,
  author={Westbrink, Fabian and Chadha, Gavneet Singh and Schwung, Andreas},
  booktitle={2018 IEEE Industrial Cyber-Physical Systems (ICPS)}, 
  title={Integrated IPC for data-driven fault detection}, 
  year={2018},
  volume={},
  number={},
  pages={277-282},
  abstract={Condition monitoring and fault detection are of critical importance in modern industrial processes for efficient and productive machine operation. Early and accurate detection of faults facilitates in the efficient and secure operation of the plant by preventing losses due to machinery breakdown. In this study, we present a novel and rapid fault detection methodology using an integrated Industrial PC. The approach is based on integrating existing hardware components and software libraries for efficient application of machine learning algorithms to an industrial process. The Industrial PC is integrated within the fieldbus system of a Bulk Good Laboratory Plant providing superior network security as compared to an external connection to a cloud platform. Additionally, the Industrial PC provides predominantly better numerical computation functionality as compared to traditional PLC environment due to the availability of machine learning libraries in high-level languages. The fault detection procedure is accomplished with the Principal Component Analysis dimensionality reduction algorithm along with Hotelling's T2 statistic. A variety of sensor fault cases pertinent to the Bulk Good Laboratory Plant are analysed and experimental results show that all fault cases were detected with low detection delays. The time required to detect faults reflects the real-time capabilities of the system in an industrial scenario.},
  keywords={Fault detection;Principal component analysis;Cloud computing;Libraries;Monitoring;Hardware;Machine learning algorithms;Integrated Industrial PC;Fault Detection;Principal Component Analysis;Bulk Good Laboratory Plant},
  doi={10.1109/ICPHYS.2018.8387672},
  ISSN={},
  month={May},}

@INPROCEEDINGS{10398108,
  author={Rai, Anjani Kumar and Pokhariya, Hemant Singh and Tiwari, Kripanshu and Divya Vani, V and Kumar, Devesh and Kumar, Arjun and Rana, Ajay},
  booktitle={2023 6th International Conference on Contemporary Computing and Informatics (IC3I)}, 
  title={IOT Driven Predictive Maintenance Using Machine Learning Algorithms}, 
  year={2023},
  volume={6},
  number={},
  pages={2674-2678},
  abstract={The modern Web of Things (IIoT) alludes to the use of Web of Things (IoT) innovation underway that empowers the use of machine information created by different sensors and uses different investigation on it to get adroit information. The data typically accompany a date and time when they are caught by the gadgets. fundamental part for predictive demonstrating. The development of the Web of Things (IoT), which gives continuous information from sensors and associated gadgets, has totally changed the modern scene. Predictive support has turned into a progressive strategy for working on the efficiency and reliability of fundamental machinery and framework here. To empower IoT-driven predictive upkeep plans, machine learning algorithms assume a basic part, which is succinctly summed up in this theoretical. To anticipate hardware disappointments and identify upkeep needs before they bring about costly breakdowns, IoT-driven predictive support utilizes the consistent stream of information from sensors and gadgets. With their ability to break down huge measures of information and recognize complex examples, machine learning algorithms have become significant to this venture.},
  keywords={Technological innovation;Machine learning algorithms;Costs;Electric breakdown;Hardware;Sensors;Task analysis;IOT-Driven;Predictive;Machine Learning;Algorithms},
  doi={10.1109/IC3I59117.2023.10398108},
  ISSN={},
  month={Sep.},}

@article{Shi2020,
  author    = {Shi, Stone},
  title     = {{INTERNATIONAL: Edge computing, artificial intelligence, automation innovation: Edge computing can help engineers lower costs and develop new applications, such as shop-floor data analysis and quality prediction. Artificial intelligence (AI) improves efficiency and accuracy}},
  journal   = {Control Engineering},
  volume    = {67},
  number    = {11},
  pages     = {8},
  month     = nov,
  year      = {2020},
  url       = {https://link.gale.com/apps/doc/A649927625/AONE?u=anon~77372379&sid=googleScholar&xid=a90843a5},
  note      = {Accessed 28 Sept. 2024}
}

@Article{info11040193,
AUTHOR = {Raschka, Sebastian and Patterson, Joshua and Nolet, Corey},
TITLE = {Machine Learning in Python: Main Developments and Technology Trends in Data Science, Machine Learning, and Artificial Intelligence},
JOURNAL = {Information},
VOLUME = {11},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {193},
URL = {https://www.mdpi.com/2078-2489/11/4/193},
ISSN = {2078-2489},
ABSTRACT = {Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical machine learning and scalable general-purpose graphics processing unit (GPU) computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.},
DOI = {10.3390/info11040193}
}

@ARTICLE{8110869,
  author={Verhelst, Marian and Moons, Bert},
  journal={IEEE Solid-State Circuits Magazine}, 
  title={Embedded Deep Neural Network Processing: Algorithmic and Processor Techniques Bring Deep Learning to IoT and Edge Devices}, 
  year={2017},
  volume={9},
  number={4},
  pages={55-65},
  abstract={Deep learning has recently become immensely popular for image recognition, as well as for other recognition and pattern matching tasks in, e.g., speech processing, natural language processing, and so forth. The online evaluation of deep neural networks, however, comes with significant computational complexity, making it, until recently, feasible only on power-hungry server platforms in the cloud. In recent years, we see an emerging trend toward embedded processing of deep learning networks in edge devices: mobiles, wearables, and Internet of Things (IoT) nodes. This would enable us to analyze data locally in real time, which is not only favorable in terms of latency but also mitigates privacy issues. Yet evaluating the powerful but large deep neural networks with power budgets in the milliwatt or even microwatt range requires a significant improvement in processing energy efficiency.},
  keywords={Feature extraction;Biological neural networks;Training data;Machine learning;Image recognition;Tutorials},
  doi={10.1109/MSSC.2017.2745818},
  ISSN={1943-0590},
  month={Fall},}

@INPROCEEDINGS{9919465,
  author={Orăşan, Ioan Lucan and Seiculescu, Ciprian and Caleanu, Cătălin Daniel},
  booktitle={2022 IEEE 16th International Symposium on Applied Computational Intelligence and Informatics (SACI)}, 
  title={Benchmarking TensorFlow Lite Quantization Algorithms for Deep Neural Networks}, 
  year={2022},
  volume={},
  number={},
  pages={000221-000226},
  abstract={Deploying deep neural network models on the resource constrained devices, e.g., lost-cost microcontrollers, is challenging because they are mostly limited in terms of memory footprint and computation capabilities. Quantization is one of the widely used solutions to reduce the size of a model. For parameter representation, it employs for example just 8-bit integer or less instead of 32-bit floating point. The TensorFlow Lite deep learning framework currently provides four methods for post-training quantization. The aim of this paper is to benchmark these quantization methods using various deep neural models of different sizes. The main outcomes of the paper are: (1) the compression ratio obtained for each quantization method for deep neural models of small, medium, and large sizes, (2) a comparison of the accuracy results relative to the original accuracy, and (3) a viewpoint for the decision to choose the quantization method depending on the model size.},
  keywords={Deep learning;Measurement;Quantization (signal);Program processors;Microcontrollers;Computational modeling;Neural networks;Deep Neural Networks;TensorFlow Lite;Post-Training Quantization},
  doi={10.1109/SACI55618.2022.9919465},
  ISSN={},
  month={May},}

@INPROCEEDINGS{10306026,
  author={Vicente, Pedro and Santos, Pedro M. and Asulba, Barikisu and Martins, Nuno and Sousa, Joana and Almeida, Luis},
  booktitle={2023 18th Conference on Computer Science and Intelligence Systems (FedCSIS)}, 
  title={Comparing Performance of Machine Learning Tools Across Computing Platforms}, 
  year={2023},
  volume={},
  number={},
  pages={1185-1189},
  abstract={Embedded systems (ES) are wide-spread in our world and responsible for many critical systems. More recently, machine learning (ML) tools have become a well-established solution for data-intensive tasks, but their application in embedded systems is still gaining traction and their real-time performance is often unclear. We provide a (non-extensive) review of the ML tools that may be suited for deployment in ES, from which we selected two representative tools - the well-established Python-based Scikit-Learn, and the interoperability-oriented ONNX Runtime - to compare their response time. Using archetypal datasets and four pre-trained ML models, we measure the prediction time for each sample, for each model, in Scikit-Learn and ONNX Runtime in a standard desktop (to compare performance of the tools in the same platform), and for ONNX Runtime in a representative ES, a Raspberry Pi v4 (to compare performance of the same tool across platforms). We report that ONNX considerably improves over Scikit-Learn, and experiences a negligible performance degradation when ported to the RPi.},
  keywords={Degradation;Runtime;Embedded systems;Economic indicators;Computational modeling;Machine learning;Predictive models;Machine Learning;Embedded Systems;Prediction Time;Scikit-Learn;ONNX Runtime},
  doi={10.15439/2023F3594},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{10603118,
  author={Carroll, Emma and Caracciolo, Giuseppe and Quintana, Sergi Gomez and Shelevytska, Viktoriya and Temko, Andriy and Popovici, Emanuel},
  booktitle={2024 35th Irish Signals and Systems Conference (ISSC)}, 
  title={AI-Driven CHD Detection Using an Ultra-Low Power Embedded System}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Congenital heart disease (CHD) is a significant global health concern, particularly in disadvantaged communities that lack access to advanced diagnostic equipment and medical ability. Capitalizing on advances in digital stethoscope technology, this paper presents an innovative AI-driven CHD detection system implemented on an ultra-low-power embedded platform. Utilizing phonocardiogram data acquired using a digital stethoscope, our solution offers accurate CHD screening, while consuming less than 5mW, enabling 24-hour operation on a small 55mAh battery. This approach enhances portability, reduces pre-screening costs, and ensures the required accuracy for early diagnosis. The model used for inference implementation is based on an Extreme Gradient Boosting (XGBoost) algorithm. The system's design and optimization for low-power environments are discussed, along with its potential of integration with low power digital stethoscope platforms.},
  keywords={Heart;Accuracy;Microcontrollers;Neural networks;Low power electronics;Stethoscope;Phonocardiography;Congenital heart disease;AI;XGBoost;phonocardiogram;neonates;optimization;embedded systems;low-power inference},
  doi={10.1109/ISSC61953.2024.10603118},
  ISSN={2688-1454},
  month={June},}

@INPROCEEDINGS{9123302,
  author={Hua, Yanpei},
  booktitle={2020 Information Communication Technologies Conference (ICTC)}, 
  title={An Efficient Traffic Classification Scheme Using Embedded Feature Selection and LightGBM}, 
  year={2020},
  volume={},
  number={},
  pages={125-130},
  abstract={Machine Learning (ML) techniques have been widely used in anomaly-based Intrusion Detection System (IDS) in the big data era. Although many advanced approaches are proposed recently, there are still several key limitations that should not be ignored. Firstly, data pre-processing methods have not gained sufficient attention, and they are vital to efficiency of model training especially with massive volume of collected samples. Secondly, in spite that deep learning is able to acquire more hidden interrelations from input data, it usually suffers from high complexity with numerous parameters tuned and much training time consumed. Thirdly, KDD99 data sets and their variants are leveraged by most of the literature for traffic classification, but they are proved to be outdated and inadequate for IDS evaluation. Therefore, to cope with these challenges, in this paper, we firstly propose a data pre-processing approach with under-sampling and embedded feature selection, in order to relieve the imbalance of traffic samples and extract dominant features of incoming flows. Then, we utilize LightGBM to build an traffic classification approach for IDS with better accuracy and efficiency. Finally, we evaluate our proposed approaches based on CIC-IDS2018, the data set issued in 2018 that contains comprehensive real network traffic. Extensive experiments are performed, and related results have confirmed the advantages of our proposed approaches over several other comparisons.},
  keywords={Feature extraction;Training;Big Data;Complexity theory;Intrusion detection;Data models;Deep learning;Intrusion Detection Systems;Feature selection;LightGBM;CIC-IDS2018},
  doi={10.1109/ICTC49638.2020.9123302},
  ISSN={},
  month={May},}

@INPROCEEDINGS{9776868,
  author={Bhavya, MR and Damodaran, Anish and Ranganath, Sindhu},
  booktitle={2022 Second International Conference on Power, Control and Computing Technologies (ICPC2T)}, 
  title={An AI based Smart Test Case Generator for Embedded Device}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={With the widespread adoption of embedded devices for IoT, there is an explosion in the number of embedded devices in various sectors such as Industrial, Consumer and Enterprises. Many of these devices now support complex concurrent use cases. This results in increased complexity of the testing eco system. Hence it is imperative that adequate test coverage is provided that maps real world use cases into test cases that covers not only the sunny day scenarios but also the rainy-day scenarios. In this paper, a Smart AI based Test Case Generator is presented that attempts to translate a use case specification document into multiple test cases that could be readily executed on the device. Further, this AI based framework is also equipped to generate new test cases based on the existing test cases.},
  keywords={Measurement;Deep learning;Computers;Biological system modeling;Power control;Generators;Explosions;AI;Smart Test case generator;IoT;Use Case Specification document},
  doi={10.1109/ICPC2T53885.2022.9776868},
  ISSN={},
  month={March},}

@INPROCEEDINGS{10303419,
  author={Zahan, Mashiat},
  booktitle={2023 International Conference on Information and Communication Technology for Sustainable Development (ICICT4SD)}, 
  title={Prediction of Faults in Embedded Software Using Machine Learning Approaches}, 
  year={2023},
  volume={},
  number={},
  pages={352-356},
  abstract={Software industry owns the most modern technological developments in this era. A software should be working perfectly without any error before handed over to the customer. The process of this quality assurance should be cost effective, less time-consuming, and reliable. But in practical scenario, it takes time to predict, detect and rectify the faults in a software. There are many models to predict the faults of software but no single model can claim to be perfect. This paper studied seven Machine Learning (ML) algorithms: Ada-Boost, CatBoost, LightGBM, Random Forest, XGBoost, Ensemble using Stacking and Voting to predict faults for an embedded software. Experiments were carried out with six datasets and eight performance metrics: accuracy, sensitivity, specificity, F-measure, balance, AUC, MAE, and precision to observe the performance of the algorithms. Random Forest showed the best results for accuracy, precision and specificity for 35%, 25% and 20% test data ratio with prediction of 0.995, 1 and 0.996 respectively. On the other hand, Ensemble (Stacking) showed the best result for specificity for 35% test ratio with prediction of 0.996. These results proved the suggested model better than the previous ones with acceptable outcome.},
  keywords={Machine learning algorithms;Costs;Fault detection;Stacking;Software algorithms;Predictive models;Prediction algorithms;Embedded;Performance Metrics;AdaBoost;Ensemble},
  doi={10.1109/ICICT4SD59951.2023.10303419},
  ISSN={},
  month={Sep.},}
@INPROCEEDINGS{9730351,
  author={Lim, Seung-Ho and Kang, Shin-Hyeok and Ko, Byeong-Hyun and Roh, Jaewon and Lim, Chaemin and Cho, Sang-Young},
  booktitle={2022 IEEE International Conference on Consumer Electronics (ICCE)}, 
  title={Architecture Exploration and Customization Tool of Deep Neural Networks for Edge Devices}, 
  year={2022},
  volume={},
  number={},
  pages={1-2},
  abstract={Recently, Deep Neural Network(DNN)-based applications are increasing in embedded edge devices. However, due to the high computational complexity, DNN has limitations in properly executing and optimizing on edge devices. As s result, DNN exploration framework is required to customize various DNN models for edge device from the software to hardware perspectives. In this paper, we provide a GUI-based framework for architectural exploration of DNN networks in edge devices. It provides software optimization such as quantization and pruning, as well as hardware performance analysis using Virtual Platform(VP)-based Deep Learning Accelerator(DLA).},
  keywords={Deep learning;Quantization (signal);Conferences;Neural networks;Computer architecture;Software;Hardware;DNN;Inference Engine;ONNX;DLA},
  doi={10.1109/ICCE53296.2022.9730351},
  ISSN={2158-4001},
  month={Jan},}

@online{arewelearningyet,
    author    = {{Are We Learning Yet?}},
    title     = {Are We Learning Yet?},
    year      = {2024},
    url       = {https://www.arewelearningyet.com/},
    note      = {Zugriff am 29. September 2024}
}

@online{ACMDigitalLibrary,
    author    = {{ACM Digital Library}},
    title     = {ACM Digital Library},
    year      = {2024},
    url       = {https://dl.acm.org/},
    note      = {Zugriff am 25. Octorber 2024}
}

@online{SpringerLink,
    author    = {{Springer Link}},
    title     = {Springer Link},
    year      = {2024},
    url       = {https://link.springer.com/},
    note      = {Zugriff am 25. Octorber 2024}
}

@misc{BeckhoffWebsite,
  author       = {{Beckhoff Automation}},
  title        = {Company - Beckhoff Automation},
  year         = {2024},
  url          = {https://www.beckhoff.com/en-gb/company/},
  note         = {Accessed: 20 October 2024}
}

@INPROCEEDINGS{10407700,
  author={Barbalho, Pedro I. N. and Coury, Denis V. and Lacerda, Vinicius A. and Fernandes, Ricardo A. S.},
  booktitle={2023 IEEE PES Innovative Smart Grid Technologies Europe (ISGT EUROPE)}, 
  title={Hardware-in-the-loop Testing of a Deep Deterministic Policy Gradient Algorithm as a Microgrid Secondary Controller}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={Microgrids play an important role in distribution system modernization by coordinating distributed energy resources and local loads. Microgrids require well-designed adaptive controllers to ensure stability in all operating points, e.g., reinforcement learning algorithms. However, recently published papers have proposed reinforcement learning-based controllers only on a simulation basis, without showing results regarding the controller performance in hardware implementation. Thus, this paper focuses on a deep deterministic policy gradient-based microgrid secondary controller in a hardware-in-the-loop setup. Its performance was analyzed and compared with the grid’s standard controller, comprised by classical tools. The DDPG controller implemented in hardware reduced the steady-state error, increased the system speed response, and reduced deviations caused by the events.},
  keywords={Europe;Microgrids;Power system stability;Hardware;Steady-state;Standards;Testing;hardware-in-the-loop;microgrids;reinforcement learning;secondary control},
  doi={10.1109/ISGTEUROPE56780.2023.10407700},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{10384901,
  author={Jesica, N and Hema Rani, P and Rajesh Kannan, C and Gandhi, Vasudevan},
  booktitle={2023 11th National Power Electronics Conference (NPEC)}, 
  title={Hardware-in-loop setup for SoC estimation of Lithium-ion batteries using Simulink Desktop Real Time}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Battery management systems (BMS) are protective circuits used to maintain the safe operation of the battery under all working conditions. The BMS also helps in cell balancing by bleeding off excess energy from high cells to bring all the cells down to the same state of charge (SoC). Evaluation of the short-term and long-term performance of a BMS is difficult, as it requires connecting it to a physical high-voltage battery, which is expensive and potentially unsafe. Additionally, it’s worth noting that the voltage of a Lithium-ion battery increases as the number of series connected cells increases. In the work presented here, Hardware-in-the-loop testing is performed on the BMS Electronic Control Unit (ECU), which has a lot more advantages than direct testing. Lithium-ion batteries are modeled in Simulink, which outputs signals exactly as compared to a real plant to test the faults occurring in the batteries. The controller model (SoC estimation algorithm) is also modeled in Simulink to perform Model-in-loop tests, followed by Software-in-loop and Processorin-loop. Finally, the SoC estimation algorithm using Extended Kalman Filter (EKF) and Machine Learning is validated using a Hardware-in-the-loop (HIL) system and the time required to compute one iteration is compared.},
  keywords={Lithium-ion batteries;Machine learning algorithms;Software packages;Computational modeling;Machine learning;Ions;Real-time systems;Hardware in Loop;Battery Management System;Lithium ion Batteries},
  doi={10.1109/NPEC57805.2023.10384901},
  ISSN={2831-4379},
  month={Dec},}

@online{IEEE_Xplore,
  author       = {{IEEE}},
  title        = {IEEE Xplore Digital Library},
  year         = {2024},
  url          = {https://ieeexplore.ieee.org/Xplore/home.jsp},
  note         = {Accessed: 20 September 2024}
}

@online{MLFlow,
  author       = {{MLFlow}},
  title        = {MLFlow},
  year         = {2024},
  url          = {https://mlflow.org/docs/latest/index.html},
  note         = {Accessed: 25 October 2024}
}

@inproceedings{10.1145/3674912.3674918,
author = {Celebioglu, Cansu and Kilickaya, Sertac and Eren, Levent},
title = {Smartphone-based Bearing Fault Diagnosis in Rotating Machinery using Audio Data and 1D Convolutional Neural Networks},
year = {2024},
isbn = {9798400716843},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674912.3674918},
doi = {10.1145/3674912.3674918},
abstract = {Asynchronous machines are essential components that drive critical systems across industrial, trading, and residential sectors, powering heating units, pumps, and various appliances. Yet, ensuring their reliable process is paramount to prevent costly defects and maintain productivity. Notably, failures in the rolling element bearings (REB) account for about forty percent of motor failures, underscoring the urgency of early detection to mitigate operational risks and financial losses. To address this challenge, this paper proposes an innovative smartphone-based diagnostic technique for detecting bearing faults in induction machines. Leveraging the common availability and computational capabilities of smartphones, the approach utilizes the devices’ audio recording functionality to capture motor audio signals. Audio data collected from rotating machines with various fault types is used to train a 1D Convolutional Neural Network (1D CNN), and the trained model is then deployed on a smartphone for real-time fault diagnosis. Embedding this approach into a user-friendly mobile application enhances accessibility and usability, offering a cost-effective solution for fault diagnosis in induction machines.},
booktitle = {Proceedings of the International Conference on Computer Systems and Technologies 2024},
pages = {149–154},
numpages = {6},
keywords = {Condition-based Monitoring (CBM), Convolutional Neural Networks (CNNs), Embedded Machine Learning, Rolling Element Bearings (REBs)},
location = {Ruse, Bulgaria},
series = {CompSysTech '24}
}

@inproceedings{10.1145/3664647.3681449,
author = {Wu, Junran and Chen, Xueyuan and Li, Shangzhe},
title = {Uncovering Capabilities of Model Pruning in Graph Contrastive Learning},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3681449},
doi = {10.1145/3664647.3681449},
abstract = {Graph contrastive learning has achieved great success in pre-training graph neural networks without ground-truth labels. Leading graph contrastive learning follows the classical scheme of contrastive learning, forcing model to identify the essential information from augmented views. However, general augmented views are produced via random corruption or learning, which inevitably leads to semantics alteration. Although domain knowledge guided augmentations alleviate this issue, the generated views are domain specific and undermine the generalization. In this work, motivated by the firm representation ability of sparse model from pruning, we reformulate the problem of graph contrastive learning via contrasting different model versions rather than augmented views. We first theoretically reveal the superiority of model pruning in contrast to data augmentations. In practice, we take original graph as input and dynamically generate a perturbed graph encoder to contrast with the original encoder by pruning its transformation weights. Furthermore, considering the integrity of node embedding in our method, we are capable of developing a local contrastive loss to tackle the hard negative samples that disturb the model training. We extensively validate our method on various benchmarks regarding graph classification via unsupervised and transfer learning. Compared to the state-of-the-art (SOTA) works, better performance can always be obtained by the proposed method.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {6510–6519},
numpages = {10},
keywords = {graph classification, graph contrastive learning, model pruning},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@inproceedings{10.1145/3368826.3377912,
author = {Cowan, Meghan and Moreau, Thierry and Chen, Tianqi and Bornholt, James and Ceze, Luis},
title = {Automatic generation of high-performance quantized machine learning kernels},
year = {2020},
isbn = {9781450370479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368826.3377912},
doi = {10.1145/3368826.3377912},
abstract = {Quantization optimizes machine learning inference for resource constrained environments by reducing the precision of its computation. In the extreme, even single-bit computations can produce acceptable results at dramatically lower cost. But this ultra-low-precision quantization is difficult to exploit because extracting optimal performance requires hand-tuning both high-level scheduling decisions and low-level implementations. As a result, practitioners settle for a few predefined quantized kernels, sacrificing optimality and restricting their ability to adapt to new hardware. This paper presents a new automated approach to implementing quantized inference for machine learning models. We integrate the choice of how to lay out quantized values into the scheduling phase of a machine learning compiler, allowing it to be optimized in concert with tiling and parallelization decisions. After scheduling, we use program synthesis to automatically generate efficient low-level operator implementations for the desired precision and data layout. We scale up synthesis using a novel reduction sketch that exploits the structure of matrix multiplication. On a ResNet18 model, our generated code outperforms an optimized floating-point baseline by up to 3.9\texttimes{}, and a state-of-the-art quantized implementation by up to 16.6\texttimes{}.},
booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Code Generation and Optimization},
pages = {305–316},
numpages = {12},
keywords = {machine learning, quantization, synthesis},
location = {San Diego, CA, USA},
series = {CGO '20}
}

@inproceedings{10.1145/3613904.3642109,
author = {Hohman, Fred and Kery, Mary Beth and Ren, Donghao and Moritz, Dominik},
title = {Model Compression in Practice: Lessons Learned from Practitioners Creating On-device Machine Learning Experiences},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642109},
doi = {10.1145/3613904.3642109},
abstract = {On-device machine learning (ML) promises to improve the privacy, responsiveness, and proliferation of new, intelligent user experiences by moving ML computation onto everyday personal devices. However, today’s large ML models must be drastically compressed to run efficiently on-device, a hurtle that requires deep, yet currently niche expertise. To engage the broader human-centered ML community in on-device ML experiences, we present the results from an interview study with 30 experts at Apple that specialize in producing efficient models. We compile tacit knowledge that experts have developed through practical experience with model compression across different hardware platforms. Our findings offer pragmatic considerations missing from prior work, covering the design process, trade-offs, and technical strategies that go into creating efficient models. Finally, we distill design recommendations for tooling to help ease the difficulty of this work and bring on-device ML into to more widespread practice.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {645},
numpages = {18},
keywords = {Efficient machine learning, design directions, interactive systems, interview study, model compression, on-device machine learning},
location = {Honolulu, HI, USA},
series = {CHI '24}
}